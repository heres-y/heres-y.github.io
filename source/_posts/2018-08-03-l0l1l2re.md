---
title: 正则项之L0L1L2
date: 2018-08-03 21:54:00
updated: 2018-08-03 21:54:00
comments: true
tags:
- 深度学习
- XJBX

categories: [深度学习]
copyright: true
urlname: l0l1l2re

---

**注意，本文讨论的是L0L1L2范数作为[代价函数/损失函数/目标函数/期望风险](https://blog.csdn.net/heyongluoyao8/article/details/52462400)中[正则项/结构风险](http://www.datalearner.com/blog/1051509805091522)的应用，而不是作为经验风险项的应用。**

在经验风险中，**L1(MAE)L2(MSE)**两者常被用来约束数据的逻辑回归。虽然同样用于逻辑回归，但终归是有区别滴(存在即合理嘛)，两者之间的[不同](http://baijiahao.baidu.com/s?id=1603857666277651546&wfr=spider&for=pc)可以总结为**MSE计算方便利于模型的学习，MAE对异常点具有更高的鲁棒性但在损失较小时仍有较大梯度，求解效率低。**

下面详细介绍L0L1L2范数以及它们作为正则项的应用。

<!-- more -->


一般正则项是以下公式的形式：

$$
\sum_{n=1}^{N} 
\left.
\middle\|
t_n-\bf w^T \phi(\bf x_n)
\middle\|
\right.^k
+
\lambda
\sum_{j=1}^M
\left.
\middle\|
w_j
\middle\|
\right.^q
$$

### L0范数

L0范数是指向量中非0的元素的个数。

使用L0范数的意义在于希望向量Ｗ中的大部分元素都是0，即**让W是稀疏的**。(关于稀疏的概念可以看[稀疏编码－Sparse coding](此处链接稀疏编码))


需要注意的是：
1.L0很难优化求解(NP难问题)
2.虽然“稀疏性”的最直接测度标准是 "L0" 范式，但这是不可微的，而且通常很难进行优化，所以普遍做法是采用L0的最优凸近似L1范数来替代L0。

所以我们才有了L1范数～



### L1范数
L1范数是指向量中各个元素的绝对值之和,又称曼哈顿距离，马氏距离（Manhattan distance）。

L1正则算子可以实现稀疏，因此又被称为“稀疏规则算子”。而在应用中，也多采用L1正则项实现模型的稀疏性。那么问题来了，L0正则算子具有明显的稀疏约束，而为什么要使用L1正则算子却不用L0呢？

主要原因有两个，一是L1可以确实实现稀疏约束，二是L0不可导难以优化求解。

关于L1为什么能够实现稀疏，简单解释是因为**任何的规则化算子，如果它在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。**，直观一点讲的话，**L1范数更加倾向于选择一些数目较少的较大的值(换句话讲选择较多的零)。**而从优化/先验知识/数值计算的角度更为详细地去理解L1的稀疏性，请移步[『科学计算』L0、L1与L2范数理解](https://www.cnblogs.com/hellcat/p/7979711.html#_label0)。

<div align = center>
<img src = "./2018-08-03-l0l1l2re/laplace_distribution.png" width ="50%" alt = "数值计算角度解释L1范数的稀疏性" />
</div>





这里插一句，如果数据集中某些特征值很大，而经验风险使用L1的话，L1倾向于选择更大的特征，这些特征会掩盖其他特征间的邻近关系。


### L2范数

L2范数是指向量中各元素的平方和然后开根号。在回归中，被称作“岭回归”(Ridge Regression)，也叫“权值衰减”(Weight Decay)，可以理解为欧几里得距离(Euclid distance)。

在正则项中，**L2范数的作用主要是用于防止数据过拟合，提升模型的泛化能力，同时改善ill-posed问题。**

<!--由于采用了欧式距离的定义，所以-->

L2为什么会防止过拟合？
L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，W中会出现许多接近0但不是0的元素(如果是0就变成L1了)，这些特征都会接近于0，这些小的权重参量会削弱数据中某些特征值较大的数据的作用，使得模型的泛化性能更好。。

对于改善ill-posed的问题，从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。

为什么L2会改善ill-posed的问题呢？
简单地说一下，系统的解析解可以表示为：
$$\hat w = \left(X^TX\right)^{-1}X^T\rm y $$
当系统是ill-conditioned时，样本数量小于样本的维度，从解方程的角度说方程个数小于未知量个数，矩阵$X^TX$是奇异矩阵，即$X^TX$不可逆，所以$w$是不可求的。L2正则在此处就大显神威了，加上L2正则项之后，解变成了：
$$\hat w = \left(X^TX-\lambda I\right)^{-1} X^T\rm y$$

上面这个式子是可以求逆的，并且$\lambda$的引入会改善condition number，从而改善了ill-posed的问题，与此同时，求解的收敛速度也增加了。







### 参考资料
[1] [深度学习——L0、L1及L2范数](https://blog.csdn.net/zchang81/article/details/70208061)
[2] [曼哈顿距离，欧式距离，余弦距离](https://blog.csdn.net/qingyang666/article/details/61919381)
<span id = "[3]"> [3] [机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995)</span>
[4] [机器学习——几种距离度量方法比较](https://my.oschina.net/hunglish/blog/787596#h2_11)
[5] [正则项的理解之正则从哪里来](http://www.datalearner.com/blog/1051509805091522)
[6] [【直观详解】什么是正则化](https://charlesliuyx.github.io/2017/10/03/%E3%80%90%E7%9B%B4%E8%A7%82%E8%AF%A6%E8%A7%A3%E3%80%91%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%88%99%E5%8C%96/)
[7] [机器学习大牛最常用的5个回归损失函数，你知道几个？](http://baijiahao.baidu.com/s?id=1603857666277651546&wfr=spider&for=pc)