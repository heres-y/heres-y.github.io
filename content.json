{"meta":{"title":"Heres-y's Blog","subtitle":"欢迎来到Heres-y的博客小站","description":"Looking for your reply~","author":"Heres-y","url":"http://heres-y.github.io","root":"/"},"pages":[{"title":"About Me","date":"2018-03-31T02:40:53.000Z","updated":"2018-10-18T14:16:04.326Z","comments":false,"path":"about/index.html","permalink":"http://heres-y.github.io/about/index.html","excerpt":"","text":"一个研究生"},{"title":"Categories","date":"2018-03-31T02:43:25.000Z","updated":"2018-03-31T04:02:41.688Z","comments":false,"path":"categories/index.html","permalink":"http://heres-y.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-03-31T02:42:48.000Z","updated":"2018-03-31T04:02:54.700Z","comments":false,"path":"tags/index.html","permalink":"http://heres-y.github.io/tags/index.html","excerpt":"","text":""},{"title":"guestbook","date":"2019-06-18T08:16:54.000Z","updated":"2019-06-18T08:50:28.238Z","comments":true,"path":"guestbook/index.html","permalink":"http://heres-y.github.io/guestbook/index.html","excerpt":"","text":""}],"posts":[{"title":"论文笔记|Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior","slug":"paper-Hyperspectral-Image-Reconstruction-Using-a-Deep-Spatial-Spectral-Prior-CVPR-2019","date":"2019-06-18T15:07:00.000Z","updated":"2019-06-19T12:50:23.594Z","comments":true,"path":"2019/06/18/HSI_deep_spatial_spectral_prior.html","link":"","permalink":"http://heres-y.github.io/2019/06/18/HSI_deep_spatial_spectral_prior.html","excerpt":"写在前面：此文发于CVPR2019 题目：Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior作者：Lizhi Wang1 Chen Sun1 Ying Fu1 Min H. Kim2 Hua Huang11Beijing Institute of Technology 2Korea Advanced Institute of Science and Technology","text":"写在前面：此文发于CVPR2019 题目：Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior作者：Lizhi Wang1 Chen Sun1 Ying Fu1 Min H. Kim2 Hua Huang11Beijing Institute of Technology 2Korea Advanced Institute of Science and Technology 正则化是求解病态优化问题的一个基本方法，并且在高光谱图像重建中得到了广泛应用。但是以往基于正则化的方法往往需要手工设计且对变化范围大的场景不鲁棒。作者提出了一种结合数据驱动先验和基于优化网络的高光谱图像重建方案，在仿真和实物系统上达到了较高精度。 为了解决欠定重建问题，可以使用正则化引入图像先验，如全变分(TV)，sparsity,non-local similarity等正则化方法。但基于经验设计的正则化方法对多样的自然光谱图像处理能力差，需要手工微调参数且往往不能求取闭合解。作者提出近期提出的基于神经网络的压缩感知方法，LISTA,ADMM-NET,ISTA-NET等虽然可以避免迭代求解，但是仍然继承了稀疏先验这一特点，限制了一些层内的特征是稀疏的，这对于求解优化问题是不利的。同时，现有基于网络的重建方法只考虑了空间维度而忽略了光谱图维度，这相当于白白损失了一个维度的先验。 作者结合优化的内在结构(structure insight of the optimization)和神经网络出色的先验建模能力，1、首先学习先验的 正则化描述子。2、将描述子与优化方法结合表示成网络形式。 HQS半二次方分裂首先需要介绍HQS算法： 有目标函数如下，其中$x$为待恢复图像。$$\\hat{x}=\\arg\\min_x\\frac{1}{2}|y-Hx|+\\lambda\\Phi(x)$$ 引入辅助变量$z$,$$\\hat{x}=\\arg\\min_x\\frac{1}{2}|y-Hx|^2+\\lambda\\Phi(z),s.t.~~z=x$$ 于是得到惩罚函数： $$\\mathcal{L_\\mu}(x,z)=\\frac{1}{2}|y-Hx|+\\lambda\\Phi(z)+\\frac{\\mu}{2}|z-x|^2$$ 于是问题转化为迭代求解以下两式： $$x_{k+1}=\\arg\\min_x|y-Hx|+\\mu|x-z_k|^2$$ $$z_{k+1} = \\arg\\min_z\\frac{\\mu}{2}|z-x_{k+1}|^2+\\lambda\\Phi(z)$$ 对应于本文要解决的压缩感知问题，当正则项不可微时，使用分离变量方法把正则项解耦出来，本文使用了HQS方法。如图1所示，对应的网络模型 ![cassimodel](./paper_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019/eq1.png) 图1 从公式(7)(8)的角度考虑，可HQS方法分离了观测模型$\\Phi$和超参先验$R(\\cdot)$，所以可以使用卷积神经网络学习一个$S(\\cdot)$来代替公式(8)的求解，如图2公式： ![cassimodel](./paper_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019/eq2.png) 图2 自然而然，一个卷积神经网络可以代替图2，网络结构如图3所示。 ![priornet](paper_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019/priornet.png) 图3 此网络的优点在于：1、考虑了空间维度和光谱维度；2、简单易学好训练。 ![cassimodel](./paper_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019/cassimodel.png) 优化与重建结合的方法与分离变量并进行迭代的方式相比，作者提出的方法整体考虑了观测模型和图像先验。图1公式(7)中的f-子问题是一个最小二成问题，可以给出闭合解： ![cassimodel](./paper_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019/eq3.png) 利用共轭梯度法(conjugate gradient, CG)可以求解f-子问题，易推出(11)(12), ![cassimodel](./paper_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019/eq4.png) ![cassimodel](./paper_Hyperspectral_Image_Reconstruction_Using_a_Deep_Spatial-Spectral_Prior_CVPR_2019/eq5.png) 将先验生成网络嵌入到(12)，得到下图的网络。 最终可以端到端训练进行重建。取得了STOA的结果。 Reference机器学习&amp;数据挖掘笔记_12（对Conjugate Gradient 优化的简单理解） HQS——Half Quadratic Splitting半二次方分裂 Learning Deep CNN Denoiser Prior for Image Restoration","categories":[{"name":"文章阅读","slug":"文章阅读","permalink":"http://heres-y.github.io/categories/文章阅读/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://heres-y.github.io/tags/论文笔记/"},{"name":"光谱","slug":"光谱","permalink":"http://heres-y.github.io/tags/光谱/"}]},{"title":"小记|Valse2019","slug":"Valse2019","date":"2019-04-15T15:35:21.000Z","updated":"2019-04-15T15:35:21.000Z","comments":true,"path":"2019/04/15/Valse2019.html","link":"","permalink":"http://heres-y.github.io/2019/04/15/Valse2019.html","excerpt":"OverviewValse 2019 本次valse注册会议的人数达到了5000+，现场也异常火爆，许多workshop/tutorial开场前10min会场就已经坐满。而第一天的三维与深度学习的workshop则因为场地太小，主办方在已经开讲的情况下转移会场。这场汇集了华人CV青年学者的研讨会是当前深度学习与CV领域的一个缩影，异常火爆与拥挤，但也百花齐放，成果丰硕，不断碰撞出新的花火。 本届valse主题非常丰富，几乎包含了计算机视觉领域的所有热点，以弱监督、迁移学习、元学习、模型与网络设计、医学影像、三维重建为代表的主题是学界关注的重点。从公司workshop和展区来看，当前工业界并不怎么关注弱监督、迁移学习等概念，业界更关注CV与AI的直接产业化，如轻量级网络的设计，网络模型搜索，行人识别、语义分析、三维场景地图等与产品更近的技术）。","text":"OverviewValse 2019 本次valse注册会议的人数达到了5000+，现场也异常火爆，许多workshop/tutorial开场前10min会场就已经坐满。而第一天的三维与深度学习的workshop则因为场地太小，主办方在已经开讲的情况下转移会场。这场汇集了华人CV青年学者的研讨会是当前深度学习与CV领域的一个缩影，异常火爆与拥挤，但也百花齐放，成果丰硕，不断碰撞出新的花火。 本届valse主题非常丰富，几乎包含了计算机视觉领域的所有热点，以弱监督、迁移学习、元学习、模型与网络设计、医学影像、三维重建为代表的主题是学界关注的重点。从公司workshop和展区来看，当前工业界并不怎么关注弱监督、迁移学习等概念，业界更关注CV与AI的直接产业化，如轻量级网络的设计，网络模型搜索，行人识别、语义分析、三维场景地图等与产品更近的技术）。 会议的很多workshop是同步进行的，所以有必要根据自己的兴趣主动做出取舍。当然，有时是因为会场爆满“被动”去听另一个完全听不懂的workshop。印象较深与略有收获主要集中在第一天与最后一天：第一天workshop“三维视觉与深度学习”，最后一日“深度学习模型设计”和poster环节。其他诸如“meta-learning”“弱监督语义分割”“single net work do more without retraining”等，我没有相关基础或因讲者进度过快推介自己的工作，听下来如蜻蜓点水，收获不大。 首先说下三维视觉的workshop。三维视觉是计算机视觉中最基础也是最经典的问题。因为转场等原因，我是从第二位国防科大的徐凯老师开始听的（后来听J.Y Peng说第一个百度的一直在打广告，没听也不是太遗憾）。徐所讲的是通过hierarchial信息进行三维重建，介绍的工作很多，时间跨度较大，从2011年的工作一直到最近的工作，核心的思路是通过图模型对3D图像进行建模，关键是如何使用合适的结点表述以及连接关系对三维图像进行结构描述，徐在此基础上完成了从二维图像和恢复三维物体，并使用auto-encoder等结构完成对三维信息的重建、理解（分割）和生成。 第三位是清华的刘烨斌老师，主题是三维人体重建。关于人体的重建更多地是应用于娱乐方面，例如VR，影视、游戏、广告等（这时相关的炫酷应用吸引了观众的眼球）。人体动态重建的目标是能够达到大规模便捷实时的精准采集与重建。刘老师科普了这方面工作的发展：1、基于多视立体几何的方法具有重建质量高、支持任意拓扑等优点，但其实时性较差；2、基于三维模板的方法可以降低求解时间；3、基于统计模板的方法无任何人工预处理同时具有语义信息，但难以重建复杂的几何拓扑；4、而基于表面动态融合的方法无需人工预处理，甚至仅需单视点，但是以来实时深度相机或者深度计算性能，对于快速运动的人体效果差。随后介绍了他们的工作：单深度相机融合结合体态模板实时重建（double fusion，2+4），效果鲁棒性好，支持复杂拓扑结构和复杂纹理。（据我了解后来在double fusion的基础上将单深度相机扩展为多深度相机…）介绍的第二个工作是单深度相机语义化动态三维重建（MulayCap），视频展示效果仍旧酷炫。。。最后刘老师给出了未来三维人体重建领域的展望（如下ppt）。 下一位是自动化所模式国重的申抒含老师，报告更加科普。把三维场景重建的流程附加代表性结果讲了一遍，包括稀疏重建、稠密重建、语义建模、矢量建模等，最后介绍了几个应用。重建效果好的可怕。 最后一位是港科大的沈劭劼老师，最初在会议手册看到这个HKUT和报告题目“…drones”就猜测是大疆做的，ppt出来后果不其然，论文中有Dji的参与。内容是介绍利用无人机进行三维场景重建，更贴近SLAM。本来slam创新研究的门槛就已经较高了，这种结合无人机的工作更加困难，没有get到点。 最后一日poster环节主要和曹讯老师的“hyperspectral imaging with random printed mask(cvpr2019)”的作者进行了讨论（讲解的是二作）。这篇文章的主要想法是通过在普通rgb相机前增加一个随机打印的彩色掩模来复原rgb图像，也是通过神经网络来恢复31个band的高光谱图像。相比于直接从rgb图像恢复高光谱，这种增加了随机色彩的掩模的成像系统具有更加准确的光谱精度。可以理解为增加了光谱基，使得恢复的高光谱图像精度更高。这个掩模是采用普通彩色打印机打印的，打印的图案为随机的彩色墨点，通过不同颜色墨点的混合，可以得到更多的色彩。个人感觉这是对于掩模式光谱相机的改进，通过与讲者的交流，在实现过程中有很多细节，实现这个系统还是要经过相当的积累与工作的。 这篇文章作者的另一篇工作“spectral reconstruction from dispersive blur: approaching full light throughput spectral imager”，通过分光镜获取灰度图像和因光路导致边缘blur的光谱图像，进行光谱重建。这篇文章通过图模型证明了实验可行性，没有看懂，且这篇文章和光学联系更加紧密。 总结与感受： 1、poster现场来看，tracking和reid独占两排，来势汹汹。基于single Image的xx 的poster也较多. 2、弱监督也需增加关注（两个弱监督workshop） 3、强化学习并没有想象中的火热，只有极少数的文章提到了强化，强化学习的威力还没有展现出来。 4、更少的信息输入（single image/view），更高维度的输出（3D）。三维相关领域是个可选方向，但是需要长时间的耕耘。 5、了解了新概念meta-learning，感觉还在算法起步阶段。 6、图模型和神经网络结合可能是未来图像相关算法的发展方向 7、自己仍需努力。","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/tags/图像处理/"},{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"}]},{"title":"Linux运行OpenGL","slug":"opengl","date":"2018-11-24T14:38:00.000Z","updated":"2019-06-18T14:37:39.005Z","comments":true,"path":"2018/11/24/opengl1.html","link":"","permalink":"http://heres-y.github.io/2018/11/24/opengl1.html","excerpt":"写在前面：在linux下开发OpenGL。简单介绍OpenGL并安装，编写第一个程序。 环境：Ubuntu 16.04 LTSGraphics:GT 730gcc 5.4.0","text":"写在前面：在linux下开发OpenGL。简单介绍OpenGL并安装，编写第一个程序。 环境：Ubuntu 16.04 LTSGraphics:GT 730gcc 5.4.0 什么是OpenGLOpenGL 是一套由SGI公司发展出来的绘图函式库，它是一组 C 语言的函式，用于 2D 与 3D 图形应用程式的开发上。OpenGL 让程式开发人员不需要考虑到各种显示卡底层运作是否相同的问题，硬体由 OpenGL 核心去沟通，因此只要显示卡支援 OpenGL，那么程式就不需要重新再移植，而程式开发人员也不需要重新学习一组函式库来移植程式。 安装1.首先安装编译器与基本函数库，一般情况下系统均已安装： 1$ sudo apt-get install build-essential 2.其次安装 OpenGL Library 1$ sudo apt-get install libgl1-mesa-dev 3.安装OpenGL Utilities 1$ sudo apt-get install libglu1-mesa-dev OpenGL Utilities 是一组建构于 OpenGL Library 之上的工具组，提供许多很方便的函式，使 OpenGL 更强大且更容易使用。 4.安装OpenGL Utility Toolkit 1$ sudo apt-get install libglut-dev OpenGL Utility Toolkit 是建立在 OpenGL Utilities 上面的工具箱，除了强化了 OpenGL Utilities 的不足之外，也增加了 OpenGL 对于视窗介面支援。 注意：在这一步的时候，可能会出现以下情况，shell提示： 1234Reading package lists... DoneBuilding dependency treeReading state information... DoneE: Unable to locate package libglut-dev 将上述$ sudo apt-get install libglut-dev命令改成$ sudo apt-get install freeglut3-dev即可。 第一个OpenGL程序代码： 123456789101112131415161718192021222324252627282930313233343536#include &lt;GL/glut.h&gt;void init(void)&#123; glClearColor(0.0, 0.0, 0.0, 0.0); glMatrixMode(GL_PROJECTION); glOrtho(-5, 5, -5, 5, 5, 15); glMatrixMode(GL_MODELVIEW); gluLookAt(0, 0, 10, 0, 0, 0, 0, 1, 0); return;&#125;void display(void)&#123; glClear(GL_COLOR_BUFFER_BIT); glColor3f(1.0, 0, 0); glutWireTeapot(3); glFlush(); return;&#125;int main(int argc, char *argv[])&#123; glutInit(&amp;argc, argv); glutInitDisplayMode(GLUT_RGB | GLUT_SINGLE); glutInitWindowPosition(0, 0); glutInitWindowSize(300, 300); glutCreateWindow(&quot;OpenGL 3D View&quot;); init(); glutDisplayFunc(display); glutMainLoop(); return 0;&#125; 编译程序： 1$ gcc -o test test.c -lGL -lGLU -lglut 执行： 1$ ./test 结果： ![result](2018-11-24-opengl/opengl1.png) 期间编译时报错： 12345678910111213141516171819test.c: In function ‘main’:test.c:30:5: error: stray ‘\\342’ in program glutCreateWindow(“OpenGL 3D View”); ^test.c:30:5: error: stray ‘\\200’ in programtest.c:30:5: error: stray ‘\\234’ in programtest.c:30:32: error: invalid suffix &quot;D&quot; on integer constant glutCreateWindow(“OpenGL 3D View”); ^test.c:30:25: error: ‘OpenGL’ undeclared (first use in this function) glutCreateWindow(“OpenGL 3D View”); ^test.c:30:25: note: each undeclared identifier is reported only once for each function it appears intest.c:30:32: error: expected ‘)’ before numeric constant glutCreateWindow(“OpenGL 3D View”); ^test.c:30:32: error: stray ‘\\342’ in programtest.c:30:32: error: stray ‘\\200’ in programtest.c:30:32: error: stray ‘\\235’ in program 解决方法：把glutCreateWindow(&quot;OpenGL 3D View&quot;);中的中文“”换成英文&quot;&quot;。 参考：Stray ‘\\342’ in C++ program Reference本文内容主要参考Ubuntu 安装OpenGLUbuntu14.04下配置OpenGL及测试代码","categories":[{"name":"OpenGL","slug":"OpenGL","permalink":"http://heres-y.github.io/categories/OpenGL/"}],"tags":[{"name":"OpenGL","slug":"OpenGL","permalink":"http://heres-y.github.io/tags/OpenGL/"}]},{"title":"CUDA程序开发","slug":"CUDA","date":"2018-11-16T14:38:00.000Z","updated":"2019-06-18T14:37:16.449Z","comments":true,"path":"2018/11/16/CUDA.html","link":"","permalink":"http://heres-y.github.io/2018/11/16/CUDA.html","excerpt":"写在前面：GPU并行计算和CUDA程序开发及优化 课堂笔记+课程作业主要是一些小程序练手","text":"写在前面：GPU并行计算和CUDA程序开发及优化 课堂笔记+课程作业主要是一些小程序练手 GPU相关知识GPU的计算模式在异构协同处理计算模型中将CPU与GPU结合起来加以利用。应用程序的串行部分在CPU上运行，而计算任务繁重的部分则由GPU的高性能计算来进行。从用户的角度来看，应用程序只是运行得更快了，获得了很好的性能提升。 高性能计算机的 分类单指令流单数据流（ SISD）• 单指令流多数据流（ SIMD）• 多指令流单数据流（ MISD）• 多指令流多数据流（ MIMD） 单独的高性能计算节点主要分为：• 同构节点（仅采用CPU， Intel Xeon CPU、 AMD Opteron CPU）• 异构节点（分为主机端和设备端，分别注重逻辑运算和浮点计算。 主流异构节点类型包括CPU+GPU和CPU+MICMIC： Many Integrated Core （Intel 集成众核） GPU硬件架构NVIDIA不同架构产品不同GPU架构的设计理念、工艺水平等均不相同，相应的内部体系结构和性能也不相同。每一构架都对应大量产品 Tesla Fermi Kepler Maxwell Pascal Volta GPU体系结构相关术语 SP（ Streaming Processor） :流处理器是GPU运算的最基本计算单元。 SFU（ Special Function Unit） :特殊函数单元用来执行超越函数指令，比如正弦、余弦、平方根等函数。 Shader core（渲染核/着色器）， SP的另一个名称，又称为CUDA core，始于Fermi架构 DP （双精度浮点运算单元） SM（ Streaming Multiprocessors） :流式多处理器是GPU架构中的基本计算单元，也是GPU性能的源泉，由 SP、DP、 SFU等运算单元组成。这是一个典型的阵列机，其执行方式为SIMT（单指令多线程），区别于传统的 SIMD（单指令流多数据流），能够保证多线程的同时执行 SPA（ Scalable streaming Processor Array）可扩展的流处理器阵列：所有处理核心和高速缓存的总和，包含所有的SM、 TPC、 GPC。与存储器系统共同组成GPU构架 MMC（ MeMory Controller）存储控制器：控制存储访问的单元，合并访存。每个存储控制器可以支持一定位宽的数据合并访存。 ROP（ raster operation processors）光栅操作单元 LD/ST（ Load/Store Unit）存储单元 ![deviceQuery](./2018-11-16-CUDA/deviceQuery.png) deviceQuery 从图中可以看出，我的GPU为GT730，CUDA版本为9.0，GPU显存为2G，拥有384个CUDA核，GPU最大时钟频率为0.9Ghz，显存带宽为64位，每个block最多支持1024个线程。 与1080TI/k20相比，有些参数没差多少。 ReferenceCSDN解读 RCAN Image Super-Resolution Using Very Deep Residual Channel Attention Networks-ECCV2018","categories":[{"name":"CUDA","slug":"CUDA","permalink":"http://heres-y.github.io/categories/CUDA/"}],"tags":[{"name":"CUDA","slug":"CUDA","permalink":"http://heres-y.github.io/tags/CUDA/"}]},{"title":"论文笔记|Unsupervised Monocular Depth Estimation with Left-Right Consistency","slug":"paper-monodepth","date":"2018-11-07T09:10:00.000Z","updated":"2019-06-18T14:39:03.496Z","comments":true,"path":"2018/11/07/mono_depth.html","link":"","permalink":"http://heres-y.github.io/2018/11/07/mono_depth.html","excerpt":"写在前面：此文发于CVPR2017，并在项目主页公布了代码。本文中文翻译：读Unsupervised Monocular Depth Estimation with LeftRight Consistency代码中文解读：[读源码] Unsupervised Monocular Depth Estimation with Left-Right Consistency关于近几年单目深度估计的文章，可以参考知乎用户的回答。 题目：Unsupervised Monocular Depth Estimation with Left-Right Consistency作者：Clement Godard Oisin Mac Aodha Gabriel J. BrostowUniversity College London","text":"写在前面：此文发于CVPR2017，并在项目主页公布了代码。本文中文翻译：读Unsupervised Monocular Depth Estimation with LeftRight Consistency代码中文解读：[读源码] Unsupervised Monocular Depth Estimation with Left-Right Consistency关于近几年单目深度估计的文章，可以参考知乎用户的回答。 题目：Unsupervised Monocular Depth Estimation with Left-Right Consistency作者：Clement Godard Oisin Mac Aodha Gabriel J. BrostowUniversity College London 背景motivation思考：人眼可以做到单目的原因是什么？ 回答：人眼单目深度估计是基于极强的先验，这也限制了单目深度估计的应用场景。 “利用图像重建误差（image reconstruction loss）来最小化光度误差（类似于SLAM中的直接法）虽可以得到很好地图像重建结果（disparity），但得到深度预测结果非常差。”（不太了解SLAM直接法是什么） 本文主要讲无监督学习的方式估计深度。基本思想基于立体匹配中的左右一致性，即将一幅图warp到另一幅图定义loss。 保留问题： 单目无监督深度的开山作是Depth Map Prediction from a Single Image using a Multi-Scale Deep Network（NIPS2014），第一篇CNN-based来做单目深度估计的文章。 other works Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural network(ECCV2016),J. Xie, R. Girshick, and A. Farhadi.计算出每个像素的视差的概率分布，对像素所在行的所有视差进行加权求和，权重即为概率。 Unsupervised CNN for single view depth estimation: Geometry to the rescue.(ECCV2016),R. Garg, V. Kumar BG, and I. Reid. 贡献 新的loss，end2end的unsupervised monocular depth estimation网络 应用在了新的数据集上 对train loss和图像生成模型进行了评估。图像生成模型是指生成视差图的模型。 主要内容网络结构： encoder-decoder结构，包括长skip。通过一个带scale的sigmoid函数，将输出限制在(0,0.3*当前scale下的图像宽度)。作者采用的激活为ELU（参看深度学习(1)-深度学习中的核函数（激活函数））。decoder结构中，作者没有用广泛采用的deconvolution结构，而是用一个最近邻升采样+后续的卷积层的方式来做分辨率提升。作者采用Adam模式（参见cs31n_lesson6_7/5.调参）来进行训练。 和一般视觉深度学习训练一样，数据增强（data augmentation）是必要的。 作者也尝试将encoder替换成Resnet50，而其他部分不变。 LOSS： $$ C_s=\\alpha_{ap}(C^l_{ap}+C^r_{ap})+\\alpha_{ds}(C^l_{ds}+C^r_{ds})+\\alpha_{lr}(C^l_{lr}+C^r_{lr}) $$ $C_{ap}$:纹理loss，与输入图像纹理相关。$\\alpha=0.8$$C_{ap}$来自于Loss Functions for Neural Networks for Image Processing,也是主要起作用的部分！ $C_{ds}$:平滑loss直观理解，原图I梯度越大的地方，要求视差图d梯度越大；原图I梯度越小的地方（平滑），允许视差图d梯度越小（平滑）。 $C_{lr}$:左右一致性loss 速度35 ms for a 512*256 image on one modern GPU 吐槽【作者在估计深度的过程中确实没有用到激光或者结构光的GT，但仍需要已知基线和焦距的双目图像对作为训练输入，测试时只需将单张图作为输入。感觉monocular有点牵强】 补充阅读 Dispnet &amp; FCN，本文使用的网络结构 STN ，作者warp图像的主要方式 本文提到另一个工作DeepStereo：从多个邻近视角选取像素合成新视角的图像。Deepstereo: Learning to predict new views from the world’s imagery. (CVPR2016)J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Reference论文笔记-深度估计(5)Unsupervised Monocular Depth Estimation with Left-Right Consistency","categories":[{"name":"文章阅读","slug":"文章阅读","permalink":"http://heres-y.github.io/categories/文章阅读/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://heres-y.github.io/tags/论文笔记/"},{"name":"深度信息","slug":"深度信息","permalink":"http://heres-y.github.io/tags/深度信息/"}]},{"title":"论文笔记：SR总结","slug":"paper-SR-summary","date":"2018-11-07T09:10:00.000Z","updated":"2018-11-15T11:51:26.274Z","comments":true,"path":"2018/11/07/SR.html","link":"","permalink":"http://heres-y.github.io/2018/11/07/SR.html","excerpt":"写在前面：这是一篇不太完善的CNN超分辨总结，整理了近年来深度学习在超分辨领域比较有代表性的工作，随缘更新。","text":"写在前面：这是一篇不太完善的CNN超分辨总结，整理了近年来深度学习在超分辨领域比较有代表性的工作，随缘更新。 SRCNN (有码) 论文： Learning a Deep Convolutional Network for Image Super-Resolution, ECCV2014 Image Super-Resolution Using Deep Convolutional Networks, TPAMI2015 项目主页：http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html作者：Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. VDSR 论文：Accurate Image Super-Resolution Using Very Deep Convolutional Networks, CVPR2016代码： official: https://cv.snu.ac.kr/research/VDSR/ pytorch: https://github.com/twtygqyy/pytorch-vdsr tensorflow: https://github.com/Jongchan/tensorflow-vdsr 作者：Jiwon Kim Jung Kwon Lee Kyoung Mu Lee ESPCN论文：Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network，CVPR2016代码：https://github.com/Tetrachrome/subpixel作者：Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken,Rob Bishop, Daniel Rueckert, Zehan Wang SRGAN 论文：Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, CVPR2017代码： github(tensorflow): https://github.com/zsdonghao/SRGAN github(tensorflow): https://github.com/buriburisuri/SRGAN github(torch): https://github.com/junhocho/SRGAN github(caffe): https://github.com/ShenghaiRong/caffe_srgan github(tensorflow): https://github.com/brade31919/SRGAN-tensorflow github(keras): https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks github(pytorch): https://github.com/ai-tor/PyTorch-SRGAN作者：Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham,Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi（Twitter的十一个作者，和deepmind的二十几个作者还有差距:) ） RCAN论文：RCAN Image Super-Resolution Using Very Deep Residual Channel Attention Networks-ECCV2018代码：github(official-pytorch)https://github.com/yulunzhang/RCAN作者：Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu ReferenceGAN相关：SRGAN，GAN在超分辨率中的应用从SRCNN到EDSR，总结深度学习端到端超分辨率方法发展历程A collection of high-impact and state-of-the-art SR methodsSuper-Resolution via Deep LearningCSDN解读 RCAN Image Super-Resolution Using Very Deep Residual Channel Attention Networks-ECCV2018","categories":[{"name":"文章阅读","slug":"文章阅读","permalink":"http://heres-y.github.io/categories/文章阅读/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://heres-y.github.io/tags/论文笔记/"},{"name":"超分辨","slug":"超分辨","permalink":"http://heres-y.github.io/tags/超分辨/"}]},{"title":"论文笔记|DEEP3D","slug":"paper-Deep3d","date":"2018-11-07T09:10:00.000Z","updated":"2019-06-14T02:09:01.466Z","comments":true,"path":"2018/11/07/DEEP3D.html","link":"","permalink":"http://heres-y.github.io/2018/11/07/DEEP3D.html","excerpt":"写在前面：Deep3d,由2D到3D视频转换，非常经典的视角重建。有码。 论文：Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks， 作者：Junyuan Xie, Ross Girshick, Ali FarhadiUniversity of Washington","text":"写在前面：Deep3d,由2D到3D视频转换，非常经典的视角重建。有码。 论文：Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks， 作者：Junyuan Xie, Ross Girshick, Ali FarhadiUniversity of Washington 所以我们的方法并不是先预测一张深度图，然后用这张深度图通过一个单独的算法去重建缺失的视角，而是在同一神经网络中重新创建端到端的方法来训练它。 ReferenceDeep3D 中文翻译及阅读笔记开源|如何使用CNN将视频从2D到3D进行自动转换（附源代码）","categories":[{"name":"文章阅读","slug":"文章阅读","permalink":"http://heres-y.github.io/categories/文章阅读/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://heres-y.github.io/tags/论文笔记/"},{"name":"深度估计","slug":"深度估计","permalink":"http://heres-y.github.io/tags/深度估计/"}]},{"title":"论文笔记：Zoom and Learn:Generalizing Deep Stereo Matching to Novel Domains","slug":"paper-Zoom-and-Learn","date":"2018-10-22T07:57:00.000Z","updated":"2019-06-18T14:38:27.863Z","comments":true,"path":"2018/10/22/zoom_and_learn_stereo_domin.html","link":"","permalink":"http://heres-y.github.io/2018/10/22/zoom_and_learn_stereo_domin.html","excerpt":"写在前面：CVPR2018，针对双目匹配在训练和实际应用中存在的domin transfer问题。由于商业原因，作者只公布了测试代码。 ![phone](paper_Zoom and Learn/phone.png) 题目：Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains作者：Jiahao Pang$^1$ Wenxiu Sun$^1$ ChengxiYang$^1$ JimmyRen$^1$ RuichaoXiao$^1$ JinZeng$^1$ LiangLin$^{1,2}$$^1$SenseTime Research $^2$Sun Yat-sen University","text":"写在前面：CVPR2018，针对双目匹配在训练和实际应用中存在的domin transfer问题。由于商业原因，作者只公布了测试代码。 ![phone](paper_Zoom and Learn/phone.png) 题目：Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains作者：Jiahao Pang$^1$ Wenxiu Sun$^1$ ChengxiYang$^1$ JimmyRen$^1$ RuichaoXiao$^1$ JinZeng$^1$ LiangLin$^{1,2}$$^1$SenseTime Research $^2$Sun Yat-sen University 背景motivation立体匹配通用难点：基于CNN的立体匹配的一个难点在于训练得到的立体匹配模型往往不能使用在真实场景(new domain)中，即两个domain之间存在gap(这也是CNN的一个通病)。 两个常识： 预训练好的模型在新domain直接使用会在边界出现artifacts 使用up-sampled的图像对生成disparity会带来额外的细节 基于这两个常识，作者使用迭代优化的方式在 让CNN得到高空间分辨率的输出(常识2) 的同时，利用graph Laplacian regularization保留边界且平滑边界的artifacts(常识1). 作者在手机拍摄的日常场景和KITTI上进行了实验验证。 other works监督： DispNet,first end-to-end CNN stereo matching CRL:A two-stage convolutional neural network for stereo matching. (ICCVW2017) GC-NET:End-to-end learning of geometry and context for deep stereo regression.（ICCV2017） DRR:Detect, replace, refine: Deep structured prediction for pixel wise labeling.(CVPR2017) 半监督： Unsupervised monocular depth estimation with left-right consistency(CVPR2017) Semi-supervised deep learning for monocular depth map prediction.(CVPR2017) Self-supervised learning for stereo matching with self-improving ability.arXiv Unsupervised learning of stereo matching.(ICCV2017)‘ 目前常用方式，合成数据集上训练后再在有GT的目标数据上finetune. End-to-end learning of geometry and context for deep stereo regression.（ICCV2017） A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation.(CVPR2016) A two-stage convolutional neural network for stereo matching.(ICCVW2017) Graph Laplacian regularization. 这里引用集中在denosing方面，不看了。 Iterative regularization/filtering 引用为image restoration方面，作者将其引入到CNN。 作者还专门强调，不同于堆叠CNN，这里的iteration是在训练过程中的iteration. 主要内容尺度多样性。 算法 ![Algorithm](paper_Zoom and Learn/Algorithm.png) 评价指标定量评价这里的评价指标很有意思。由于作者在三个数据集上进行了验证，有一个是和训练集相同的FlyingThings3D-80,另外一个是没有GT的手机拍摄的数据集，还有KITTI2015。 ![performace1](paper_Zoom and Learn/performance1.png) ![performace2](paper_Zoom and Learn/performance2.png) EPE:End-point-End3ER:three-pixel error rate这里所谓的PSNR和SSIM是利用disparity和右图生成的左图与真正的左图计算的PSNR和SSIM。左右图warp后肯定会有occlusion的存在，不知道这么PSNR意义是否很大。 视觉指标 ![result1](paper_Zoom and Learn/result1.png) ![result2](paper_Zoom and Learn/result2.png) 总结总结起来，使用了graph Laplacian 保持物体的边界，具体方法迭代patch。大概就这样。与另一篇相似的工作Unsupervised Adaptation for Deep Stereo相比，指标高了那么一diudiu。 相关工作Unsupervised Adaptation for Deep Stereo代码：tensorflow版本、caffe版本 文章有何贡献：本文提出了一种新的 fine-tuning 的方法使在大量合成数据上训练的 DispNet 可以迁移到无 groudtruth 或者只有极少量的 groundtruth 的实际数据集上。 本文研究的问题有何价值：双目深度估计的标签现实中很难获得，本文提出的 fine-tuning 方法可以在没 有groundtruth 的情况下将模型迁移过来。 所研究问题有何困难：如何获得可靠的监督信息来 fine-tune。 本文的解决思路是怎样的：文章受在 Kitti 数据集上 fine-tune 的启发，发现利用稀疏的标签也可以很好地对模型进行训练。文章利用传统算法如 AD-CENSUS 或 SGM 生成 label 来作为 groundtruth， 同时利用 CCCN（一种 confidence measure 的方法）来选取可信度高的 label，只利用这部分置信度高的 sparse label 来 fine-tune。 Reference本期最新 9 篇论文，每一篇都想推荐给你 | PaperDaily #14","categories":[{"name":"文章阅读","slug":"文章阅读","permalink":"http://heres-y.github.io/categories/文章阅读/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://heres-y.github.io/tags/论文笔记/"},{"name":"深度信息","slug":"深度信息","permalink":"http://heres-y.github.io/tags/深度信息/"}]},{"title":"论文笔记：Pyramid Stereo Matching Network","slug":"paper-PSM-Net","date":"2018-10-18T08:40:00.000Z","updated":"2019-06-18T14:11:28.458Z","comments":true,"path":"2018/10/18/PSM_Net.html","link":"","permalink":"http://heres-y.github.io/2018/10/18/PSM_Net.html","excerpt":"[//]写在前面：此文发于CVPR2018，并公布了代码 题目：Pyramid Stereo Matching Network作者：Jia-Ren Chang, Yong-Sheng ChenDepartment of Computer Science, National Chiao Tung University, Taiwan","text":"[//]写在前面：此文发于CVPR2018，并公布了代码 题目：Pyramid Stereo Matching Network作者：Jia-Ren Chang, Yong-Sheng ChenDepartment of Computer Science, National Chiao Tung University, Taiwan 背景motivation立体匹配通用难点：occlusion areas,repeated patterns,textureless regions,reflective surfaces.本文主攻方向：单独使用intensity-consistency导致在纹理区域表现不佳，现有立体匹配框架多基于patch-based Siamese结构且缺少context信息。从全局context信息中提取regional support. other works MC-CNN Displets: utilizes object information by modeling 3D vehicles to resolve ambiguities in stereo matching. ResMatchNet: learns to measure reflective confidence for the disparity maps to improve performance in ill-posed regions. GC-Nets: employs the encoder-decoder architecture to merge multiscale features for cost volume regularization. 贡献 提出一个无后处理的end2end的立体匹配学习框架 引入pyramid pooling模块使提取的图像特征包含全局context信息 提出 堆栈沙漏式的3D-CNN ，去扩展cost volume时的局部的上下文信息 在KITTI刷到了state-of-the-art（目前被M2S_CSPN等挤下去了，在stereo2012/2015分别排名8/20 截止2018.10.18） 主要内容提出PSM-Net，包括两个主要模块，spatital pyramid pooling 和3D-CNN.spatial pyramind pooling 在不同尺度和位置聚合了全局context信息，并形成cost volume3D-CNN使用stacked mutiple hourglass结构(来源：Stacked Hourglass Networks for Human Pose Estimation,ECCV2016)标准化cost volume并链接中间的监督。 网络结构 相关工作本文主要借鉴了两种提取全局上下文信息的网络结构：hourglass 和 pyramid pooling. 两种结构均有大量相关的工作。 ReferencePyramid Stereo Matching Network","categories":[{"name":"文章阅读","slug":"文章阅读","permalink":"http://heres-y.github.io/categories/文章阅读/"}],"tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://heres-y.github.io/tags/论文笔记/"},{"name":"深度信息","slug":"深度信息","permalink":"http://heres-y.github.io/tags/深度信息/"}]},{"title":"The volume boot has only 5.1MB disk space remaining","slug":"boot空间爆炸","date":"2018-08-16T02:32:00.000Z","updated":"2018-08-16T02:32:00.000Z","comments":true,"path":"2018/08/16/boot-space.html","link":"","permalink":"http://heres-y.github.io/2018/08/16/boot-space.html","excerpt":"/boot 目录中是系统引导文件和内核，更新内核之后旧内核还存放在里面，安装软件的时候就会提示 /boot 空间不足，佳解的决办法就是将旧内核删除。 1、查看磁盘使用情况 df -h","text":"/boot 目录中是系统引导文件和内核，更新内核之后旧内核还存放在里面，安装软件的时候就会提示 /boot 空间不足，佳解的决办法就是将旧内核删除。 1、查看磁盘使用情况 df -h 2、查看正在使用的内核版本 uname -auname -r都可以 3、查看系统中存在的内核版本 dpkg –get-selections | grep linux（当出现deibstall,则说明已经删除） 4、把低于当前版本的内核删掉 sudo apt-get remove linux-image-4.13..0-41-genericsudo apt-get autoremove linux-image-4.13..0-41-generic都可以，至于autoremove 和remove的区别，我先用remove再用autoremove实验了一下，发现remove只能删除指定的image，而autoremove则可以把这个内核相关的“linux-headers-4.xx.x-xx”等东西删掉，删的更加彻底。 5、删完之后df -h","categories":[{"name":"linux","slug":"linux","permalink":"http://heres-y.github.io/categories/linux/"}],"tags":[{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"},{"name":"linux","slug":"linux","permalink":"http://heres-y.github.io/tags/linux/"}]},{"title":"正则项之L0L1L2","slug":"l0l1l2re","date":"2018-08-03T13:54:00.000Z","updated":"2018-08-03T13:54:00.000Z","comments":true,"path":"2018/08/03/l0l1l2re.html","link":"","permalink":"http://heres-y.github.io/2018/08/03/l0l1l2re.html","excerpt":"注意，本文讨论的是L0L1L2范数作为代价函数/损失函数/目标函数/期望风险中正则项/结构风险的应用，而不是作为经验风险项的应用。 在经验风险中，L1(MAE)L2(MSE)两者常被用来约束数据的逻辑回归。虽然同样用于逻辑回归，但终归是有区别滴(存在即合理嘛)，两者之间的不同可以总结为MSE计算方便利于模型的学习，MAE对异常点具有更高的鲁棒性但在损失较小时仍有较大梯度，求解效率低。 下面详细介绍L0L1L2范数以及它们作为正则项的应用。","text":"注意，本文讨论的是L0L1L2范数作为代价函数/损失函数/目标函数/期望风险中正则项/结构风险的应用，而不是作为经验风险项的应用。 在经验风险中，L1(MAE)L2(MSE)两者常被用来约束数据的逻辑回归。虽然同样用于逻辑回归，但终归是有区别滴(存在即合理嘛)，两者之间的不同可以总结为MSE计算方便利于模型的学习，MAE对异常点具有更高的鲁棒性但在损失较小时仍有较大梯度，求解效率低。 下面详细介绍L0L1L2范数以及它们作为正则项的应用。 一般正则项是以下公式的形式： $$\\sum_{n=1}^{N}\\left.\\middle|t_n-\\bf w^T \\phi(\\bf x_n)\\middle|\\right.^k+ \\lambda\\sum_{j=1}^M\\left.\\middle|w_j\\middle|\\right.^q$$ L0范数L0范数是指向量中非0的元素的个数。 使用L0范数的意义在于希望向量Ｗ中的大部分元素都是0，即让W是稀疏的。(关于稀疏的概念可以看稀疏编码－Sparse coding) 需要注意的是：1.L0很难优化求解(NP难问题)2.虽然“稀疏性”的最直接测度标准是 “L0” 范式，但这是不可微的，而且通常很难进行优化，所以普遍做法是采用L0的最优凸近似L1范数来替代L0。 所以我们才有了L1范数～ L1范数L1范数是指向量中各个元素的绝对值之和,又称曼哈顿距离，马氏距离（Manhattan distance）。 L1正则算子可以实现稀疏，因此又被称为“稀疏规则算子”。而在应用中，也多采用L1正则项实现模型的稀疏性。那么问题来了，L0正则算子具有明显的稀疏约束，而为什么要使用L1正则算子却不用L0呢？ 主要原因有两个，一是L1可以确实实现稀疏约束，二是L0不可导难以优化求解。 关于L1为什么能够实现稀疏，简单解释是因为任何的规则化算子，如果它在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。，直观一点讲的话，L1范数更加倾向于选择一些数目较少的较大的值(换句话讲选择较多的零)。而从优化/先验知识/数值计算的角度更为详细地去理解L1的稀疏性，请移步『科学计算』L0、L1与L2范数理解。 这里插一句，如果数据集中某些特征值很大，而经验风险使用L1的话，L1倾向于选择更大的特征，这些特征会掩盖其他特征间的邻近关系。 L2范数L2范数是指向量中各元素的平方和然后开根号。在回归中，被称作“岭回归”(Ridge Regression)，也叫“权值衰减”(Weight Decay)，可以理解为欧几里得距离(Euclid distance)。 在正则项中，L2范数的作用主要是用于防止数据过拟合，提升模型的泛化能力，同时改善ill-posed问题。 L2为什么会防止过拟合？L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，W中会出现许多接近0但不是0的元素(如果是0就变成L1了)，这些特征都会接近于0，这些小的权重参量会削弱数据中某些特征值较大的数据的作用，使得模型的泛化性能更好。。 对于改善ill-posed的问题，从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。 为什么L2会改善ill-posed的问题呢？简单地说一下，系统的解析解可以表示为：$$\\hat w = \\left(X^TX\\right)^{-1}X^T\\rm y $$当系统是ill-conditioned时，样本数量小于样本的维度，从解方程的角度说方程个数小于未知量个数，矩阵$X^TX$是奇异矩阵，即$X^TX$不可逆，所以$w$是不可求的。L2正则在此处就大显神威了，加上L2正则项之后，解变成了：$$\\hat w = \\left(X^TX-\\lambda I\\right)^{-1} X^T\\rm y$$ 上面这个式子是可以求逆的，并且$\\lambda$的引入会改善condition number，从而改善了ill-posed的问题，与此同时，求解的收敛速度也增加了。 参考资料[1] 深度学习——L0、L1及L2范数[2] 曼哈顿距离，欧式距离，余弦距离 [3] 机器学习中的范数规则化之（一）L0、L1与L2范数[4] 机器学习——几种距离度量方法比较[5] 正则项的理解之正则从哪里来[6] 【直观详解】什么是正则化[7] 机器学习大牛最常用的5个回归损失函数，你知道几个？","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://heres-y.github.io/categories/深度学习/"}],"tags":[{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"},{"name":"深度学习","slug":"深度学习","permalink":"http://heres-y.github.io/tags/深度学习/"}]},{"title":"install","slug":"install","date":"2018-08-01T02:49:00.000Z","updated":"2018-08-01T02:49:00.000Z","comments":true,"path":"2018/08/01/install.html","link":"","permalink":"http://heres-y.github.io/2018/08/01/install.html","excerpt":"install MATLABUninstall MATLABmain reference:https://ww2.mathworks.cn/help/install/ug/install-and-activate-without-an-internet-connection.html 1matlabroot/bin/deactivate_matlab.sh matlabrootrepresents your own MATLAB ROOT FILE FOLDER. 12rm -rf matlabrootrm -r /home/username/.matlab","text":"install MATLABUninstall MATLABmain reference:https://ww2.mathworks.cn/help/install/ug/install-and-activate-without-an-internet-connection.html 1matlabroot/bin/deactivate_matlab.sh matlabrootrepresents your own MATLAB ROOT FILE FOLDER. 12rm -rf matlabrootrm -r /home/username/.matlab 12sudo gedit ./.bashrc//delete the imformation about matlab Install MATLABmain reference:https://blog.csdn.net/qq_16234613/article/details/78996565https://blog.csdn.net/u011961856/article/details/79644342https://blog.csdn.net/yusiguyuan/article/details/24269129 1234567mkdir /media/matlab//mount -t 类型 -o 挂接方式 源路径 目标路径sudo mount -t auto -o loop /home/username/Downloads/R2018a_glnxa64_dvd1.iso /media/matlabcd..sudo /media/matlab/installumount /media/matlab 12sudo gedit ./.bashrcexport PATH=$PATH:/usr/local/MATLAB/R2018/bin 参考资料：[1][2][3][4] install latex+sublimeinstall texlive1sudo apt-get install texlive install sublime-text1234567wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -sudo apt-get install apt-transport-httpsecho &quot;deb https://download.sublimetext.com/ apt/stable/&quot; | sudo tee /etc/apt/sources.list.d/sublime-text.listsudo apt-get updatesudo apt-get install sublime-text%% apt-get install latexmk install Package-control12345678import urllib.request,os,hashlib; h = &apos;6f4c264a24d933ce70df5dedcf1dcaee&apos; + &apos;ebe013ee18cced0ef93d5f746d80ef60&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) );by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) install dbus1234567cd ~/下载tar xzvf dbus-python-1.2.4.tar.gzsudo apt-get install libdbus-glib-1-devcd dbus-python-1.2.4./configuremakesudo make install Ubuntu安装Sublime Text 3–解决无法使用搜狗中文输入法安装完成后发现无法在Sublime输入中文，而linux常用输入法是搜狗For Linux输入法解决方法: 创建文件保存下面的代码到文件 sublime_imfix.c ,命令： 1cd ~ &amp;&amp; gedit sublime_imfix.c 代码： 1234567891011121314151617#include &lt;gtk/gtkimcontext.h&gt;void gtk_im_context_set_client_window (GtkIMContext *context, GdkWindow *window)&#123; GtkIMContextClass *klass; g_return_if_fail (GTK_IS_IM_CONTEXT (context)); klass = GTK_IM_CONTEXT_GET_CLASS (context); if (klass-&gt;set_client_window) klass-&gt;set_client_window (context, window); g_object_set_data(G_OBJECT(context),&quot;window&quot;,window); if(!GDK_IS_WINDOW (window)) return; int width = gdk_window_get_width(window); int height = gdk_window_get_height(window); if(width != 0 &amp;&amp; height !=0) gtk_im_context_focus_in(context);&#125; 拷贝保存即可～ 编译为共享库将上一步的代码编译成共享库 libsulime-imfix.so ，命令： 12cd ~gcc -shared -o libsublime-imfix.so sublime_imfix.c `pkg-config --libs --cflags gtk+-2.0` -fPIC 有些童鞋无法编译，出现错误： 123No package &apos;gtk+-2.0&apos; foundsublime_imfix.c:1:30: fatal error: gtk/gtkimcontext.h: 没有那个文件或目录 #include &lt;gtk/gtkimcontext.h&gt; 需要先安装必要的依赖： 1sudo apt-get install libgtk2.0-dev 安装完成后重新编译，可以了吧～ 修改sublime文件然后将 libsublime-imfix.so 拷贝到 sublime_text 所在文件夹，命令： 12cd ~sudo mv libsublime-imfix.so /opt/sublime_text/ 修改Sublime的命令 /usr/bin/subl 的内容: 1sudo gedit /usr/bin/subl 将文件内容： 12#!/bin/shexec /opt/sublime_text/sublime_text &quot;$@&quot; 替换修改为： 12#!/bin/shLD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text &quot;$@&quot; 完成后，在命令行中执行 subl 重启Sublime ，就可以使用搜狗For Linux的中文输入法了～ 保证多种启动方式均可使用搜狗继续图形界面快捷方式 sublime_text.desktop 的修改： sudo gedit /usr/share/applications/sublime_text.desktop 将[Desktop Entry]字段下的字符串： 1Exec=/opt/sublime_text/sublime_text %F 替换修改为： 1Exec=bash -c &quot;LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text %F&quot; 将[Desktop Action Window]字段下的字符串： 1Exec=/opt/sublime_text/sublime_text -n 替换修改为： 1Exec=bash -c &quot;LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text -n&quot; 将[Desktop Action Document]字段下的字符串： 1Exec=/opt/sublime_text/sublime_text --command new_file 替换修改为： 1Exec=bash -c &quot;LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text --command new_file&quot; 参考：http://www.qingzz.cn/ubuntu_sublime_sogou Install Code::Blocks123sudo add-apt-repository ppa:damien-moore/codeblocks-stablesudo apt updatesudo apt install codeblocks codeblocks-contrib 如何在Ubuntu 16.04/17.04上安装Code::Blockshttps://blog.csdn.net/skullsky/article/details/53134114 参考资料：[1] Install CUDA","categories":[{"name":"linux","slug":"linux","permalink":"http://heres-y.github.io/categories/linux/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/tags/图像处理/"}]},{"title":"sublime","slug":"sublime","date":"2018-07-24T07:18:00.000Z","updated":"2018-07-24T07:18:00.000Z","comments":true,"path":"2018/07/24/sublime.html","link":"","permalink":"http://heres-y.github.io/2018/07/24/sublime.html","excerpt":"","text":"快速查找与替换多数情况下，我们需要查找文中某个关键字出现的其它位置，这时并不需要重新将该关键字重新输入一遍然后搜索，我们只需要使用Shift + ←/→或Ctrl + D选中关键字，然后F3跳到其下一个出现位置，Shift + F3跳到其上一个出现位置，此外还可以用Alt + F3选中其出现的所有位置（之后可以进行多重编辑，也就是快速替换）。 大小写转换转换为大写：Ctrl+KU转换为小写：Ctrl+KL 参考资料：[1] https://www.learnopencv.com/homography-examples-using-opencv-python-c/[2] https://blog.csdn.net/xuyangcao123/article/details/70916767[3] https://www.zybuluo.com/codeep/note/163962[4] http://blog.sina.com.cn/s/blog_6965d96d0101u98s.html","categories":[{"name":"linux","slug":"linux","permalink":"http://heres-y.github.io/categories/linux/"}],"tags":[{"name":"摸鱼日志","slug":"摸鱼日志","permalink":"http://heres-y.github.io/tags/摸鱼日志/"}]},{"title":"单应性变换","slug":"homography","date":"2018-07-23T13:54:00.000Z","updated":"2018-07-23T13:54:00.000Z","comments":true,"path":"2018/07/23/homography.html","link":"","permalink":"http://heres-y.github.io/2018/07/23/homography.html","excerpt":"单应性变换(homography)所谓单应性变换就是一个平面到另一个平面的映射关系。","text":"单应性变换(homography)所谓单应性变换就是一个平面到另一个平面的映射关系。 如图，两张图片中的相同的点叫做corresponding points,比如图中红色的两点就是一对corresponding points。单应性(homography)矩阵就是表示从一张图到另一张图的映射关系的变换矩阵。 $$H=\\begin{bmatrix}h_{00} &amp; h_{01} &amp; h_{02} \\h_{10} &amp; h_{11} &amp; h_{12} \\h_{20} &amp; h_{21} &amp; h_{22}\\end{bmatrix}$$ $$ \\begin{bmatrix}x_1 \\\\ y_1 \\\\ 1\\end{bmatrix}=H\\begin{bmatrix}x_2\\\\ y_2\\\\ 1\\end{bmatrix}=\\begin{bmatrix}h_{00} &amp; h_{01} &amp; h_{02}\\h_{10} &amp; h_{11} &amp; h_{12}\\h_{20} &amp; h_{21} &amp; h_{22}\\end{bmatrix}\\begin{bmatrix}x_2\\y_2\\1\\end{bmatrix}$$ 代码实现123456789101112131415161718192021222324252627282930#!/usr/bin/env pythonimport cv2import numpy as np if __name__ == '__main__' : # Read source image. im_src = cv2.imread('book2.jpg') # Four corners of the book in source image pts_src = np.array([[141, 131], [480, 159], [493, 630],[64, 601]]) # Read destination image. im_dst = cv2.imread('book1.jpg') # Four corners of the book in destination image. pts_dst = np.array([[318, 256],[534, 372],[316, 670],[73, 473]]) # Calculate Homography h, status = cv2.findHomography(pts_src, pts_dst) # Warp source image to destination based on homography im_out = cv2.warpPerspective(im_src, h, (im_dst.shape[1],im_dst.shape[0])) # Display images cv2.imshow(\"Source Image\", im_src) cv2.imshow(\"Destination Image\", im_dst) cv2.imshow(\"Warped Source Image\", im_out) cv2.waitKey(0) 单应性变换应用最常见的当属于对文档进行自由变换。如使用手机拍摄文件或者明信片时，由于相机或拍摄角度等因素影响，拍摄的文档并不是矩形，而往往带有一定程度的畸变，此时可以使用单应性变换对文档进行自由变换矫正。此处从网上扒了一个使用PS对名片进行自由变换的例子，其本质就是单应性变换的应用。 拍摄照片： PS自由变换后： 另一个例子： 中间时代广场左上角的广告牌被替换为”Les Horribles Cernettes”的海报。 参考资料[1] https://www.learnopencv.com/homography-examples-using-opencv-python-c/[2] https://blog.csdn.net/xuyangcao123/article/details/70916767[3] markdown公式指导手册[4] http://blog.sina.com.cn/s/blog_6965d96d0101u98s.html","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/tags/图像处理/"},{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"}]},{"title":"5分钟利用Github+Hexo搭建个人博客!","slug":"github+hexo","date":"2018-06-22T06:22:05.000Z","updated":"2018-06-22T06:22:05.000Z","comments":true,"path":"2018/06/22/github+hexo.html","link":"","permalink":"http://heres-y.github.io/2018/06/22/github+hexo.html","excerpt":"前言本文记录了使用github+hexo搭建个人博客的过程以及搭建过程中遇到的一些问题。 关于使用hexo搭建博客，hexo官网已经有了很详细的介绍，但官方文档的介绍往往是力求全面详尽而显得过于冗长(尽管hexo已经够简洁了)，下面介绍如何以最快的速度实现一个“最小hexo系统”。 搭建环境为Ubuntu16.04.(Windows/Mac/Linux等其他环境请参考hexo官网)","text":"前言本文记录了使用github+hexo搭建个人博客的过程以及搭建过程中遇到的一些问题。 关于使用hexo搭建博客，hexo官网已经有了很详细的介绍，但官方文档的介绍往往是力求全面详尽而显得过于冗长(尽管hexo已经够简洁了)，下面介绍如何以最快的速度实现一个“最小hexo系统”。 搭建环境为Ubuntu16.04.(Windows/Mac/Linux等其他环境请参考hexo官网) 安装hexo 安装Git： 1$ sudo apt-get install git-core hexo是基于Node.js的静态博客，需要使用node.js里的npm工具，所以接下来下载Node.js。以下两种方式二选一即可。 12$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh$ wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh 下载完成后，重启终端并执行下列命令 1$ nvm install stable 至此，我们完成了安装hexo的准备工作。而安装hexo呢，只需一个命令： 安装hexo1$ npm install hexo-cli -g hexo安装完毕！ 发布你的第一篇文章 初始化hexo 12345$ hexo i blog //init的缩写 blog是项目名$ cd blog //切换到站点根目录$ npm install$ hexo g //generetor的缩写$ hexo s //server的缩写 创建第一篇文章 12hexo new mypaper.md//文章会生成在/blog/sorce/_post/下 本地服务器访问网页打开浏览器输入localhost:4000查看网页效果，当然，此时只是默认主题，如果不喜欢可以使用next，这是一个界面简洁且功能丰富的主题。 设置github仓库在github上新建一个repository并且务必命名为username.github.io。 修改本地blog根目录下的配置文件_config.xml: 1234deploy: type: git repo: git@github.com:username/username.github.io.git branch: master 安装插件 1$ npm install hexo-deployer-git --save 将博客部署到github上 123hexo cleanhexo g //generatehexo d //deploy 花里胡哨的东西首页只显示「摘要」在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法： 1.在文章中使用 &lt;!-- more --&gt; 手动进行截断，Hexo 提供的方式 「推荐」2.在文章的 front-matter 中添加 description，并提供文章摘录 3.自动形成摘要，在 主题配置文件 中添加： 123auto_excerpt: enable: true length: 150 默认截取的长度为 150 字符，可以根据需要自行设定 建议使用 &lt;!-- more --&gt;（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 Hexo 中的插件更好的识别。 尝试了第一二种方式，但是第二种方式在超过一行的情况下不work，还没有找到解决方法。 ————20190618更新———— 页面点击小红心将 love.js 文件添加到 \\themes\\next\\source\\js\\src 文件目录下。love.js: 1!function(e,t,a)&#123;function r()&#123;for(var e=0;e&lt;n.length;e++)n[e].alpha&lt;=0?(t.body.removeChild(n[e].el),n.splice(e,1)):(n[e].y--,n[e].scale+=.004,n[e].alpha-=.013,n[e].el.style.cssText=&quot;left:&quot;+n[e].x+&quot;px;top:&quot;+n[e].y+&quot;px;opacity:&quot;+n[e].alpha+&quot;;transform:scale(&quot;+n[e].scale+&quot;,&quot;+n[e].scale+&quot;) rotate(45deg);background:&quot;+n[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;var n=[];e.requestAnimationFrame=e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;,function(e)&#123;var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),function()&#123;var a=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;a&amp;&amp;a(),function(e)&#123;var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,n.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;),t.body.appendChild(a)&#125;(e)&#125;&#125;(),r()&#125;(window,document); 找到 \\themes\\next\\layout_layout.swing 文件， 在文件的后面， 标签之前 添加以下代码： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 背景的设置将 particle.js 文件添加到 \\themes\\next\\source\\js\\src 文件目录下。 1!function()&#123;function n(n,e,t)&#123;return n.getAttribute(e)||t&#125;function e(n)&#123;return document.getElementsByTagName(n)&#125;function t()&#123;i=a.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,c=a.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight&#125;function o()&#123;d.clearRect(0,0,i,c);var n,e,t,a,m,r,y=[x].concat(w);w.forEach(function(o)&#123;for(o.x+=o.xa,o.y+=o.ya,o.xa*=o.x&gt;i||o.x&lt;0?-1:1,o.ya*=o.y&gt;c||o.y&lt;0?-1:1,d.fillRect(o.x-.5,o.y-.5,1,1),e=0;e&lt;y.length;e++)n=y[e],o!==n&amp;&amp;null!==n.x&amp;&amp;null!==n.y&amp;&amp;(a=o.x-n.x,m=o.y-n.y,r=a*a+m*m,r&lt;n.max&amp;&amp;(n===x&amp;&amp;r&gt;=n.max/2&amp;&amp;(o.x-=.03*a,o.y-=.03*m),t=(n.max-r)/n.max,d.beginPath(),d.lineWidth=t/2,d.strokeStyle=&quot;rgba(&quot;+u.c+&quot;,&quot;+(t+.2)+&quot;)&quot;,d.moveTo(o.x,o.y),d.lineTo(n.x,n.y),d.stroke()));y.splice(y.indexOf(o),1)&#125;),l(o)&#125;var i,c,a=document.createElement(&quot;canvas&quot;),u=function()&#123;var t=e(&quot;script&quot;),o=t.length,i=t[o-1];return&#123;l:o,z:n(i,&quot;zIndex&quot;,-1),o:n(i,&quot;opacity&quot;,.5),c:n(i,&quot;color&quot;,&quot;0,0,0&quot;),n:n(i,&quot;count&quot;,99)&#125;&#125;(),m=&quot;c_n&quot;+u.l,d=a.getContext(&quot;2d&quot;),l=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(n)&#123;window.setTimeout(n,1e3/45)&#125;,r=Math.random,x=&#123;x:null,y:null,max:2e4&#125;;a.id=m,a.style.cssText=&quot;position:fixed;top:0;left:0;z-index:&quot;+u.z+&quot;;opacity:&quot;+u.o,e(&quot;body&quot;)[0].appendChild(a),t(),window.onresize=t,window.onmousemove=function(n)&#123;n=n||window.event,x.x=n.clientX,x.y=n.clientY&#125;,window.onmouseout=function()&#123;x.x=null,x.y=null&#125;;for(var w=[],y=0;u.n&gt;y;y++)&#123;var s=r()*i,f=r()*c,h=2*r()-1,g=2*r()-1;w.push(&#123;x:s,y:f,xa:h,ya:g,max:6e3&#125;)&#125;setTimeout(function()&#123;o()&#125;,100)&#125;(); 找到 \\themes\\next\\layout_layout.swing 文件， 在文件的后面，L2Dwidget.init({\"model\":\"hijiki\",\"bottom\":-30,\"log\":false,\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"pluginRootPath\":\"live2dw/\",\"tagMode\":false});标签之前 添加以下代码： 12&lt;!-- 背景动画 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/particle.js&quot;&gt;&lt;/script&gt; 给 Github 添加 README默认情况下，Github中每一个项目，我们希望有一份 README.md 的文件来作为项目的说明，但是我们在项目根目录下的 blog\\source 目录下创建一份 README.md 文件，写好说明介绍，部署的时候，这个 README.md 会被 hexo 解析掉，而不会被解析到 Github 中去的。正确的解决方法其实很简单：方法1：把 README.md 文件的后缀名改成 “MDOWN” 然后扔到blog/source文件夹下即可，这样 hexo 不会解析，Github 也会将其作为 MD 文件解析。方法2：解决方法很简单，在站点配置文件中，搜索 skip_render:，在其冒号后加一个空格然后加上 README.md 即可。 ##### 实现guestbook留言板功能 进入到博客的根目录，运行命令： 1hexo new page guestbook ##### 添加Local search功能 安装搜索插件： 1hexo-generator-searchdb 在博客根目录下执行以下命令： 1$ npm install hexo-generator-searchdb --save 配置博客安装完成，编辑博客配置文件：_config.yml 12345search: path: search.xml field: post format: html limit: 10000 配置主题Next 主题自带搜索设置，编辑主题配置文件：_config.yml 找到文件中 Local search 的相关配置，设为 true 123# Local searchlocal_search: enable: true hexo 重新部署 hexo本地图片插入关于图片插入，耗费了我一些时间。最初我是在/source/文件夹下建立了一个upload_image/用来存放图片，在markdown中调用时是![image](../../upload_image/image.png)这种形式，可以看出，需要向上跳两级目录，才能找到对应的图像。后来又尝试把图像直接放在与.md文件同一目录下，要不然是在编辑markdown时不显示，要不然是生成的静态网页不显示。找了好久终于在Hexo 图片插入找到了解决方法，需要安装 1npm install https://github.com/CodeFalling/hexo-asset-image --save 这样一个插件。方可以使用相对路径的图片，既可以在markdown中显示，也可以在网页中显示。 参考资料[1] 大道至简——Hexo简洁主题推荐[2]Hexo-NexT搭建个人博客（二）[3]Hexo 搭建博客的个性化设置二[4]Hexo NexT 主题添加留言本页面[5]hexo - Next 主题添加搜索功能","categories":[{"name":"hexo","slug":"hexo","permalink":"http://heres-y.github.io/categories/hexo/"}],"tags":[{"name":"摸鱼日志","slug":"摸鱼日志","permalink":"http://heres-y.github.io/tags/摸鱼日志/"}]},{"title":"使用神经网络进行图像超分辨踩的坑","slug":"SubpixelTrick","date":"2018-05-02T12:45:00.000Z","updated":"2018-05-02T12:45:00.000Z","comments":true,"path":"2018/05/02/SubpixelTrick.html","link":"","permalink":"http://heres-y.github.io/2018/05/02/SubpixelTrick.html","excerpt":"1、matlab的bicubic结果会有小于0或者大于255的情况出现。 Image interpolation wrong for pixel values exceeding vmax #8631MATLABresize函数中也说明了： 1234% For bicubic interpolation, the output image may have some values% slightly outside the range of pixel values in the input image. This% may also occur for user-specified interpolation kernels.%","text":"1、matlab的bicubic结果会有小于0或者大于255的情况出现。 Image interpolation wrong for pixel values exceeding vmax #8631MATLABresize函数中也说明了： 1234% For bicubic interpolation, the output image may have some values% slightly outside the range of pixel values in the input image. This% may also occur for user-specified interpolation kernels.% 2、神经网络训练出来的结果也会有小于0或者大于255的情况出现 Also remember that loss function is based on scaled images (pixel values between 0 and 1), so the predicted residual has to be scaled back by 255 before it is added to low resolution image. If some pixels become negative or go above 255, we return them to 0 and 255 respectively before saving. 解决方法：*make sure no pixels are outside [0, 255] interval* 参考1.https://www.cntk.ai/pythondocs/CNTK_302A_Evaluation_of_Pretrained_Super-resolution_Models.html2.https://cntk.ai/pythondocs/CNTK_302B_Image_Super-resolution_Using_CNNs_and_GANs.html3.https://blog.csdn.net/Autism_/article/details/794017984.https://github.com/matplotlib/matplotlib/issues/8631","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/tags/图像处理/"},{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"}]},{"title":"Life is short you need python","slug":"pythonTrick","date":"2018-04-22T08:07:00.000Z","updated":"2018-04-22T08:07:00.000Z","comments":true,"path":"2018/04/22/pythonTrick.html","link":"","permalink":"http://heres-y.github.io/2018/04/22/pythonTrick.html","excerpt":"关于if not x:&amp;if x is not None&amp;if not x is None代码中经常会有变量是否为None的判断，有三种主要的写法： 第一种是if x is None； 第二种是 if not x：； 第三种是if not x is None（这句这样理解更清晰if not (x is None)） 。 如果你觉得这样写没啥区别，那么你可就要小心了，这里面有一个坑。先来看一下代码： 123456789101112&gt;&gt;&gt; x = 1 &gt;&gt;&gt; not x False &gt;&gt;&gt; x = [1] &gt;&gt;&gt; not x False &gt;&gt;&gt; x = 0 &gt;&gt;&gt; not x True &gt;&gt;&gt; x = [0] # You don&apos;t want to fall in this one. &gt;&gt;&gt; not x False","text":"关于if not x:&amp;if x is not None&amp;if not x is None代码中经常会有变量是否为None的判断，有三种主要的写法： 第一种是if x is None； 第二种是 if not x：； 第三种是if not x is None（这句这样理解更清晰if not (x is None)） 。 如果你觉得这样写没啥区别，那么你可就要小心了，这里面有一个坑。先来看一下代码： 123456789101112&gt;&gt;&gt; x = 1 &gt;&gt;&gt; not x False &gt;&gt;&gt; x = [1] &gt;&gt;&gt; not x False &gt;&gt;&gt; x = 0 &gt;&gt;&gt; not x True &gt;&gt;&gt; x = [0] # You don&apos;t want to fall in this one. &gt;&gt;&gt; not x False 在python中 None, False, 空字符串””, 0, 空列表[], 空字典{}, 空元组()都相当于False ，即： 1not None == not False == not &apos;&apos; == not 0 == not [] == not &#123;&#125; == not () 因此在使用列表的时候，如果你想区分x==[]和x==None两种情况的话, 此时if not x:将会出现问题： 1234567891011121314151617181920&gt;&gt;&gt; x = [] &gt;&gt;&gt; y = None &gt;&gt;&gt; &gt;&gt;&gt; x is None False &gt;&gt;&gt; y is None True &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; not x True &gt;&gt;&gt; not y True &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; not x is None &gt;&gt;&gt; True &gt;&gt;&gt; not y is None False &gt;&gt;&gt; 也许你是想判断x是否为None，但是却把x==[]的情况也判断进来了，此种情况下将无法区分。对于习惯于使用if not x这种写法的pythoner，必须清楚x等于None, False, 空字符串””, 0, 空列表[], 空字典{}, 空元组()时对你的判断没有影响才行。 而对于if x is not None和if not x is None写法，很明显前者更清晰，而后者有可能使读者误解为if (not x) is None，因此推荐前者，同时这也是谷歌推荐的风格 结论： if x is not None是最好的写法，清晰，不会出现错误，以后坚持使用这种写法。 使用if not x这种写法的前提是：必须清楚x等于None, False, 空字符串””, 0, 空列表[], 空字典{}, 空元组()时对你的判断没有影响才行。 Python if 和 for 的多种写法1.not in 123456&gt;&gt;&gt; a=2&gt;&gt;&gt; a not in [2,3,4]False&gt;&gt;&gt; a in [2,3,4] True 2.c if a else b #这里注意，一定要有b,而且b不能为pass 1234567&gt;&gt;&gt; a=3 if 2&gt;3 else 4&gt;&gt;&gt; a4&gt;&gt;&gt; a=3 if 2&lt;3 else 4 &gt;&gt;&gt; a3 3.[fun(a) for a in […]] 12&gt;&gt;&gt; [a+1 for a in [2,3,4,5,6]][3, 4, 5, 6, 7] 4.a,b=b,a 1234567&gt;&gt;&gt; a=1&gt;&gt;&gt; b=2&gt;&gt;&gt; a,b=b,a&gt;&gt;&gt; a2&gt;&gt;&gt; b1 5.’内容’.join([string array]) 12345678910&gt;&gt;&gt; &apos;.&apos;.join[2,3,4,5,6] Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &apos;builtin_function_or_method&apos; object has no attribute &apos;__getitem__&apos;&gt;&gt;&gt; &apos;.&apos;.join([2,3,4,5,6]) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: sequence item 0: expected string, int found&gt;&gt;&gt; &apos;.&apos;.join([&apos;2&apos;,&apos;3&apos;,&apos;4&apos;,&apos;5&apos;,&apos;6&apos;]) &apos;2.3.4.5.6&apos; 数组顺序翻转排序：https://blog.csdn.net/u014292358/article/details/79503397OpenCV与Python之图像的读入与显示以及利用Numpy的图像转换：https://www.cnblogs.com/visionfeng/p/6094423.html # Convert from RGB -&gt; BGR input_a = input_a[..., [2, 1, 0]] input_b = input_b[..., [2, 1, 0]]###参考资料：[1] python代码if not x: 和if x is not None:和if not x is None:使用[2] [python里面的几个用法，not in，c if a else b，[fun(a) for a in […]] , a,b=b,a,’内容’.join(string array)](https://blog.csdn.net/u013176681/article/details/53995190) [3] Python if 和 for 的多种写法","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://heres-y.github.io/categories/深度学习/"}],"tags":[{"name":"摸鱼日志","slug":"摸鱼日志","permalink":"http://heres-y.github.io/tags/摸鱼日志/"},{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"}]},{"title":"光谱响应与量子效率","slug":"QE","date":"2018-04-18T14:27:00.000Z","updated":"2018-04-18T14:27:00.000Z","comments":true,"path":"2018/04/18/QE.html","link":"","permalink":"http://heres-y.github.io/2018/04/18/QE.html","excerpt":"##光谱响应光谱响应单位为A/W,直观物理意义是单位功率下产生的电流响应。 ##量子效率量子效率分为外量子效率和内量子效率一般量子效率指外量子效率，是指单位时间内外电路中产生的电子数与单位时间内的入射单色光子数之比。内量子效率的定义为：单位时间内外电路中产生的电子书与单位时间内的入射（有效）单色光子数之比。比较两个定义，可以看出内量子效应强调的是有效两字。那么无效的部分就是指因为反射或者投射等不被sensor所接收的光子了。 ##二者关系量子效率定义中，单位时间内的电子数 即为 的电流 。光谱响应定义中，也有关于电流的概念：单位功率下产生的电流。 所以，量子效率 = (1240 * 光谱响应)/响应波长 E=hv=hc/λ, h和c是常量，把数值带进去。注意：因为最后E的单位是eV，λ的单位是nm，所以在中间会涉及单位的转换。h = 6.626196×10^-34 J.s， c = 3×10^8 m/s， 上面的单位是ev，所以还要除以1.6×10^-19 c，λ的单位nm换成m还要乘以1×10^9，最后就得到1240/λ eV","text":"##光谱响应光谱响应单位为A/W,直观物理意义是单位功率下产生的电流响应。 ##量子效率量子效率分为外量子效率和内量子效率一般量子效率指外量子效率，是指单位时间内外电路中产生的电子数与单位时间内的入射单色光子数之比。内量子效率的定义为：单位时间内外电路中产生的电子书与单位时间内的入射（有效）单色光子数之比。比较两个定义，可以看出内量子效应强调的是有效两字。那么无效的部分就是指因为反射或者投射等不被sensor所接收的光子了。 ##二者关系量子效率定义中，单位时间内的电子数 即为 的电流 。光谱响应定义中，也有关于电流的概念：单位功率下产生的电流。 所以，量子效率 = (1240 * 光谱响应)/响应波长 E=hv=hc/λ, h和c是常量，把数值带进去。注意：因为最后E的单位是eV，λ的单位是nm，所以在中间会涉及单位的转换。h = 6.626196×10^-34 J.s， c = 3×10^8 m/s， 上面的单位是ev，所以还要除以1.6×10^-19 c，λ的单位nm换成m还要乘以1×10^9，最后就得到1240/λ eV 有时候会把两个概念混为一谈 光谱响应 量子效率 光谱响应指光阴极量子效率与入射波长之间的关系光谱响应表示不同波长的光子产生电子-空穴对的能力。定量地说，太阳电池的光谱响应就是当某一波长的光照射在电池表面上时，每一光子平均所能收集到的载流子数。太阳电池的光谱响应又分为绝对光谱响应和相对光谱响应。各种波长的单位辐射光能或对应的光子入射到太阳电池上，将产生不同的短路电流，按波长的分布求得其对应的短路电流变化曲线称为太阳电池的绝对光谱响应。如果每一波长以一定等量的辐射光能或等光子数入射到太阳电池上，所产生的短路电流与其中最大短路电流比较，按波长的分布求得其比值变化曲线，这就是该太阳电池的相对光谱响应。但是，无论是绝对还是相对光谱响应，光谱响应曲线峰值越高，越平坦，对应电池的短路电流密度就越大，效率也越高。 ##参考资料[1] http://muchong.com/html/201309/6391286.html[2] https://baike.baidu.com/item/%E9%87%8F%E5%AD%90%E6%95%88%E7%8E%87/3641896?fr=aladdin[3] https://www.jianshu.com/p/0864630e5fe4","categories":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"http://heres-y.github.io/tags/图像处理/"},{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"}]},{"title":"MATLAB读取曲线图并重新绘制","slug":"MatlabRePlot","date":"2018-04-08T14:30:00.000Z","updated":"2018-04-08T14:31:00.000Z","comments":true,"path":"2018/04/08/MatlabRePlot.html","link":"","permalink":"http://heres-y.github.io/2018/04/08/MatlabRePlot.html","excerpt":"最近需要从datasheet中读取某款相机的光谱响应曲线，发现下面这段代码^1。但是最后发现和自己的需求并不匹配，自己是要最终求出曲线的函数，完成由离散的点到数学公式的转换过程。这个小demo能够实现的仅仅是re-draw the pictures.仍旧记之。","text":"最近需要从datasheet中读取某款相机的光谱响应曲线，发现下面这段代码^1。但是最后发现和自己的需求并不匹配，自己是要最终求出曲线的函数，完成由离散的点到数学公式的转换过程。这个小demo能够实现的仅仅是re-draw the pictures.仍旧记之。 12345678910111213141516171819202122232425I = imread('target.png');%读取处理好的图片，必须是严格坐标轴线为边界的图片I=rgb2gray(I); %灰度变化I(I&gt;200)=255; %二值化I(I&lt;=200)=0; %二值化imshow(I) %显示图片figure;[y,x] = find(I==0); %找出曲线的像素位置y = max(y) -y; %将屏幕坐标转换为有手系迪卡坐标plot(x,y,'r.','markersize',2)%显示转换后的图像[Xx,Yy]= ginput(2); % 读取真是坐标左上角和右下角的两点min_x = min(Xx);max_x = max(Xx);min_y= min(Yy);max_y = max(Yy);% x1 = (x-Xx(1))*(max_x-min_x)/(Xx(2)- Xx(1))+min_x;% y1 = (y-Yy(1))*(min_y-max_y)/(Yy(2)- Yy(1))+max_y;%% 坐标变化，如果坐标原点不为0，则需在该轴加上省去的坐标轴数xo = 0;%原始图像起点坐标xyo = 0;%原始图像起点坐标xxl = 3; %原始图像x轴长yl=25; %原始图像y轴长x1 = (x - min(Xx))*xl/(max(Xx)-min(Xx))+xo ;%数据点x值y1 = (y - min(Yy))*yl/(max(Yy)-min(Yy))+yo;%数据点y值plot(x1,y1,'r.','markersize',2)axis([0,3,0,25]) 原图：效果图：","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://heres-y.github.io/categories/深度学习/"}],"tags":[{"name":"MATLAB","slug":"MATLAB","permalink":"http://heres-y.github.io/tags/MATLAB/"},{"name":"XJBX","slug":"XJBX","permalink":"http://heres-y.github.io/tags/XJBX/"}]},{"title":"Hello Hexo!","slug":"hello-hexo","date":"2018-03-27T06:22:05.000Z","updated":"2018-07-23T08:35:00.320Z","comments":true,"path":"2018/03/27/hello-hexo.html","link":"","permalink":"http://heres-y.github.io/2018/03/27/hello-hexo.html","excerpt":"","text":"hello, welcome!","categories":[],"tags":[{"name":"摸鱼日志","slug":"摸鱼日志","permalink":"http://heres-y.github.io/tags/摸鱼日志/"}]}]}