<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[论文笔记|Stereo Magnification_Learning view synthesis using multiplane images]]></title>
    <url>%2F2019%2F10%2F10%2Fpapers_20191010.html</url>
    <content type="text"><![CDATA[此文发于SIGGRAPH2018。 题目：Stereo Magnification: Learning view synthesis using multiplane images作者： TINGHUI ZHOU, University of California, BerkeleyRICHARD TUCKER, GoogleJOHN FLYNN, GoogleGRAHAM FYFFE, GoogleNOAH SNAVELY, Google 项目地址:https://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/ 最近需要有zz任务做视角合成相关的工作,之前对这个领域是完全不了解的.读了几篇文章,记录如下. 背景手机相机的图像质量正逐渐提升,这得益于硬件效果的提升以及HDR,散焦等摄影功能的实现.多摄像头的应用也使得手机有潜力超越传统摄影. 本文探讨的是针对用于立体视觉的双摄手机,如何对获取的双目图像扩大基线(baseline),得到新基线条件下的双目图像,作者将此称为 Stereo Magification. 不同于以往的工作,扩大基线的主要难点在于基线相当于对双目图像进行外插,而以往的基于双目图像进行视角合成的工作多是对双目之间的视角进行插值(内插).本文在外插方面做到了较好的视觉效果,且内插结果也更为流畅合理. 除此之外,本文的另一贡献是使用deep learning的方法,并制作了相关的数据集. 初次接触这个领域,下面从Related Work 中摘取一些内容,以备后记. 传统视角合成方法视角合成,即将一个或多个视角的场景作为输入,产生多个新视角的图像,是计算机图形学领域的一个经典问题,也是许多基于图像的渲染任务的核心. multiple (&gt;2) input views: 密集视角内插(light field) 稀疏视角内插 基于立体视觉的视角合成: 三维立体视频转化为多视角视频以用于裸眼多视显示 从微基线立体视觉合成4d光场 从多个小基线视角重建几何 本文关注立体视觉并针对更大的外插. 基于学习的视角合成利用深度网络强大的表征能力，视角合成问题可以用深度网络进行建模求解． - DeepStereo (cvpr2016,是利用深度学习进行视角合成的影响力较大的一篇工作) - LearningBased View Synthesis for Light Field Cameras.(siggraph asia2016)上述方法只能根据训练输出特定视角的目标场景,本文则通过预测场景的表征,利用这些表征去渲染一个范围内的视角. 其他文章对合成立体图像对,大相机运动,甚至单幅图像生成光场都有了探讨.本文针对更为复杂多样的室内外场景,并可以应用在VR设备上. 视角合成的场景表征场景的视角合成主要分为两类,一类是对每个输入的视角进行表征,然后利用插值求得目标视角的图;另一种方式是对整个场景进行表征,这种表征多为体素堆积或者”层”的形式.例如layered depth images(LDIs,1998). 与本文最为接近的是Soft 3D Reconstruction for View Synthesis.(SIGGRAPH Asia2017),他们通过显式建模置信度以达到soft的效果(本文通过透明度). 本文的Multiplane Image (MPI) 结合了多层表示和层之间的softness. 方法通过端到端的学习,本文的算法可以直接输出基于MPI的场景表征.并根据这个表征+目标视角的姿态渲染目标视角. 所谓MPI,可以参考[1]. 网络结构如图所示 输入两张图像,reference source and second source,将second source进行shift+stack(就是立体匹配中的cost volume),然后把reference source 堆上去,形成一个BHWx3(D+1)的tensor,作为输入送进网络,与此同时送入网络的还有相机的内参外参.所以网络的输入共有两组量:volume+内外参. 默认情况下,网络输出为一张rgb背景图HW3,D通道个权重图HWD,D通道个alpha图HWD,所以网络的输出参数量为HWx(2D+3),rgb背景图与权重图组合得到rgb,最终得到场景的MPI表征:一系列RGBa图像,这种方式比直接输出rgba(HWx4D)计算量要小. 得到场景的MPI表征后,在测试时根据预期目标视角的相机姿态(外参)+MPI表征就可以通过warp+alpha over compositing[2]得到预期视角的图像了. 优化函数如下图. L为loss函数,本文采用的是vgg loss. R是warp+alpha合成操作,$f_\theta$是输入图到MPI场景表征的映射.I1,I2是输入图像,c1,c2是对应的内外参数,ct,It为目标视角的内外参和合成视角的图像.c1=&lt;p1,k1&gt;.pi,ki分别为外参和内参. 具体的warp方式如下,是标准的反向warp. 数据集制作本文的另一大贡献在于数据集的制作.作者利用在youtube上扒下来的房地产视频,使用structure from motion 和SLAM的方式获取相机参数,制作了数据集.数据集的制作: 结果复现利用作者提供的model跑了一下代码,这是输出的结果.从alpha图像可以看出对每一个深度平面的估计结果是比较好的,能够明显看出从后向前的深度变化.MPI_alpha 通道的图像. MPI图像: 最后的渲染效果: 我在另一个数据集上测试了下,效果不怎么好,看起来MPI就没有估计对,猜测是相机的参数没有设置正确. Reference[1]关于Disney Multiplane camera 的介绍: https://www.bilibili.com/video/av29925039/https://en.wikipedia.org/wiki/Multiplane_camera [2]Alpha compositing: https://en.wikipedia.org/wiki/Alpha_compositing [3]国内视频：http://www.sohu.com/a/258973610_100279313 相关文章：Pratul P. Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi, and Ren Ng. 2017. Learning to Synthesize a 4D RGBD Light Field from a Single Image. In ICCV. Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. 2016. LearningBased View Synthesis for Light Field Cameras. In Proc. SIGGRAPH Asia Eric Penner and Li Zhang. 2017. Soft 3D Reconstruction for View Synthesis. In Proc. SIGGRAPH Asia. John Flynn, Ivan Neulander, James Philbin, and Noah Snavely. 2016. DeepStereo: Learning to Predict New Views From the World’s Imagery. In CVPR.]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>视角合成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|近期论文合集]]></title>
    <url>%2F2019%2F06%2F25%2Fpapers_21090625.html</url>
    <content type="text"><![CDATA[下面是最近读的几篇论文的合集，主要是AAAI2018和CVPR2019。 MSI fine grain recognition of powders Hyperspectral Imaging with Random Printed Mask non-local-global HSI denoising unsupervised cross_spectral_stereo_cycleGAN Multispectral Transfer Network MSI fine grain recognition of powders此文发于CVPR2019。 题目：Multispectral Imaging for Fine-Grained Recognition of Powders on Complex Backgrounds作者：Tiancheng Zhi, Bernardo R. Pires, Martial Hebert and Srinivasa G. NarasimhanCarnegie Mellon University urlname:MSI_powders_recongnition_cvpr2019 使用光谱图像对粉末进行分类，建立了粉末物体的数据集，方法貌似没有创新，属于工程应用。【各种粉末名词劝退】 系统搭的有点意思，使用了三个相机拍摄： 粉末举例： Hyperspectral Imaging with Random Printed Mask此文发于CVPR2019题目：Hyperspectral Imaging with Random Printed Mask作者：Yuanyuan Zhao1 Hui Guo1 Zhan Ma1 Xun Cao1 Tao Yue1,2 Xuemei Hu11Nanjing University, Nanjing, China2NJU institute of sensing and imaging engineering, Nanjing, China urlname: random_printed_HSI_cvpr2019 提出一种使用商用打印机打印掩膜的重建高光谱图像的方法。实质是对mask的改进，不过脑洞还是挺大的。也通过硬件实现，有说服力。 Overview： non-local-global HSI denoising此文发于CVPR2019 题目：Non-local Meets Global: An Integrated Paradigm for Hyperspectral Denoising作者：Wei He1, Quanming Yao2∗, Chao Li1, Naoto Yokoya1y, Qibin Zhao11RIKEN AIP 2HKUST urlname: nonlocalGlobal_HSI_denoising_cvpr2019 Multispectral Transfer Network此文发于AAAI2018 题目：Multispectral Transfer Network:Unsupervised Depth Estimation for All-Day Vision作者：Namil Kim,∗1,2 Yukyung Choi,∗1,3 Soonmin Hwang,1 In So Kweon11Korea Advanced Institute of Science and Technology (KAIST), Korea2NAVER LABS Corp., Korea 3Clova, NAVER Corp., Korea urlname: Multispectral_Transfer_Network_aaai2019 unsupervised cross_spectral_stereo_cycleGAN此文发于AAAI2019，欢迎在文末留言交流。 题目：Unsupervised Cross-spectral Stereo Matching by Learning to Synthesize作者：Mingyang Liang1;2∗, Xiaoyang Guo3∗, Hongsheng Li3, Xiaogang Wang3, You Song1 y1Beihang University, Beijing, China2SenseTime Research3The Chinese University of Hong Kong, Hong Kong, China urlname:cross_spec_stereo_cycleGAN_AAAI2019 Reference]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|近期论文合集——超分辨CVPR2019]]></title>
    <url>%2F2019%2F06%2F25%2Fpapers_21090626.html</url>
    <content type="text"><![CDATA[下面是最近读的几篇论文的合集，主要是AAAI2018和CVPR2019。 Learning a Single Convolutional Super-Resolution Network for Multiple Degradations Blind Super-Resolution With Iterative Kernel Correction Image Super-Resolution by Neural Texture Transfer Feedback Network for Image Super-Resolution Dual Residual Networks Leveraging the Potential of Paired Operations for Image Restoration “Double-DIP” :Unsupervised Image Decomposition via Coupled Deep-Image-Priors Unprocessing Images for Learned Raw Denoising Learning Parallax Attention for Stereo Image Super-Resolution Enhancing the Spatial Resolution of Stereo Images using a Parallax Prior Meta-SR此文发于CVPR2019 题目：Meta-SR: A Magnification-Arbitrary Network for Super-Resolution作者：Xuecai Hu∗1,2 , Haoyuan Mu∗ 4, Xiangyu Zhang3, Zilei Wang1, Tieniu Tan1,2, Jian Sun31 University of Science and Technology of China2 Center for Research on Intelligent Perception and Computing, NLPR, CASIA3 Megvii Inc (Face++) 4 Tsinghua University urlname:meta_sr_cvpr2019 使用meta-learning解决单模型连续尺度超分辨的的问题。主要贡献是提出一个meta upscale 模块，根据放缩尺度r学习了不同的卷积核，但是具体如何实现上采样操作，矩阵乘法如何实现的没有搞懂，需要看代码。 网络结构如下图所示： Learning a Single Convolutional Super-Resolution Network for Multiple Degradations此文发于CVPR2018 题目：Learning a Single Convolutional Super-Resolution Network for Multiple Degradations作者：Kai Zhang1;2;3, Wangmeng Zuo1, Lei Zhang21School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China2Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China3DAMO Academy, Alibaba Group 作者提出了一种可以handle不同降质模型的网络；维度拉伸策略处理LR图像/模糊和/噪声水平之间的不匹配问题。先用PCA把模糊和和噪声水平到和LR图像一样大小。 然后卷积+pixelshuffle恢复HR。 处理方式较为粗暴。 Blind Super-Resolution With Iterative Kernel Correction此文发于CVPR2019 题目：Blind Super-Resolution With Iterative Kernel Correction作者：Jinjin Gu1∗, Hannan Lu2∗, Wangmeng Zuo2, Chao Dong31The School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen2School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China3ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab,Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences urlname: nonlocalGlobal_HSI_denoising_cvpr2019 本文提出一种迭代核相关方法来估计超分辨问题中的模糊核。文章清晰易懂，值得一读。整体结构如下： h为估计出的模糊核。 Image Super-Resolution by Neural Texture Transfer此文发于CVPR2019，代码 题目： Image Super-Resolution by Neural Texture Transfer作者：Zhifei Zhang Zhaowen Wang Zhe Lin Hairong QiADOBE工作。 基本思想是patch match，从Ref图像中找到与LR最像的patch进行引导，不过由于Ref和LR是unpair的，所以匹配和融合都是在特征层面上操作。 下面是纹理转换模块的内部结构，其实就是resnet+upsample. 关于数据集制作，使用了网络搜索的图作为Ref。工作量有，提供了数据集，很棒。 至于结果，因为提供了ref图，所以可以从ref图中获取原有的LR图中几乎不可能恢复的信号，如下图的美国国旗，即使是SRGAN也不能从LR中恢复，但是本文由于有Ref，可以从Ref中获取极强的先验恢复美国国旗。（让我想到了华为P30拍月亮）。 Loss有四个部分：mse，vggLOSS，生成loss，还有作者自己设计的纹理loss(这个应该是借鉴的STYLE tranfer)。 idea很自然，实验也充足。总的来说，不错的一篇工作。 Feedback Network for Image Super-Resolution此文发于CVPR2019，代码 题目： Feedback Network for Image Super-Resolution作者：Zhen Li1 Jinglei Yang2 Zheng Liu3 Xiaomin Yang1∗ Gwanggil Jeon4 Wei Wu1∗1Sichuan University, 2University of California, Santa Barbara, 3University of British Columbia,4Incheon National University 和2017cvpr Feedback networks很像。使用带约束的RNN隐状态作为反馈，课程学习策略进行训练。 所谓curriculum learning 就是先训练简单的数据，再训练复杂的数据。对应到本文，即对每一步的loss分配权重。 本文网络结构，看看2017cvpr Feedback networks就明白了，作者把原文的ConvLSTM替换为了Feedback Block(FB). 这就是作者设计的FB模块： 总的来说，本文让我了解了Feedback Network和curriculum learning.Hhhh Dual Residual Networks Leveraging the Potential of Paired Operations for Image Restoration此文发于CVPR2019 题目： Dual Residual Networks Leveraging the Potential of Paired Operations for Image Restoration作者：Xing Liuy Masanori Suganumayz Zhun Sunz Takayuki OkataniyzyGraduate School of Information Sciences, Tohoku University zRIKEN Center for AIP 看到文章第一幅图猜想是篇以实验取胜的论文，嗯，可以参考简书:文章学习40“Dual Residual Networks Leveraging the Potential of Paired Operations for Image Restoration” 总的来说，网络结构加加减减，实验很足，效果很好。 “Double-DIP” :Unsupervised Image Decomposition via Coupled Deep-Image-Priors此文发于CVPR2019 题目：“Double-DIP” :Unsupervised Image Decomposition via Coupled Deep-Image-Priors作者：：Yossi Gandelsman Assaf Shocher Michal IraniDept. of Computer Science and Applied MathematicsThe Weizmann Institute of Science, Israel 这篇文章的基础是CVPR2018 Deep Image Prior.论文解读 没什么好说的。 Unprocessing Images for Learned Raw Denoising此文发于CVPR2019 题目：Unprocessing Images for Learned Raw Denoising作者：Tim Brooks1 Ben Mildenhall2 Tianfan Xue1Jiawen Chen1 Dillon Sharlet1 Jonathan T. Barron11Google Research, 2UC Berkeley 提出图像逆处理,将图像根据成像过程转换为调色、gamma调节、白平衡、色域转换等流程，通过直接处理原始图像传感器的原始测量数据来去噪。Darmstadt Noise数据集上得到了14%-38% 的提升和 9×-18×训练加速。 Learning Parallax Attention for Stereo Image Super-Resolution此文发于CVPR2019 题目：Learning Parallax Attention for Stereo Image Super-Resolution作者：Longguang Wang1, Yingqian Wang1, Zhengfa Liang2, Zaiping Lin1, Jungang Yang1, Wei An1, Yulan Guo1∗1College of Electronic Science and Technology, National University of Defense Technology, China2National Key Laboratory of Science and Technology on Blind Signal Processing, China 首先使用Res_ASPP提取特征,然后使用PAM(parallax attention module)计算相似性并对齐，最后聚合特征上采样得到SR。 重点关注其PAM的设计以及loss设计。 PAM 具体设计如下：经过residual aspp模块得到的两路特征A,B，特征A经过一个残差模块后过1x1conv得到特征Q，另一路特征B经过残差模块后过1x1conv得到特征S，Q’S相乘再过一个softmax得到并行注意力映射图Mba。这里把Mab中值较大的结果认为是非遮挡部分，与特征图O concat在一起进行fusion。 作者设计了4个loss，除了基本的mseloss之外，增加了warploss，约束ATTENtion map平滑的差分loss，以及warpwarp loss(作者称其为cycle loss，即warp过去再warp回来算loss)。 PAM的attention MAP 与cost volume具有很高的相似度。作者在此证明了PAM相比于带家具和有更小的参数两，更快的计算速度，更好的效果。(此实验结果设置为本文cnn结构中只替换PAM为cost volume) Reference]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|Multispectral Imaging for Fine-Grained Recognition of Powders on Complex Backgrounds]]></title>
    <url>%2F2019%2F06%2F24%2FHS_FineGrain_cvpr2019.html</url>
    <content type="text"><![CDATA[写在前面：此文发于CVPR2019，欢迎在文末留言交流。 题目：Multispectral Imaging for Fine-Grained Recognition of Powders on Complex Backgrounds作者：Tiancheng Zhi, Bernardo R. Pires, Martial Hebert and Srinivasa G. NarasimhanCarnegie Mellon University此文略读，没细看。 本文使用光谱图像对粉末进行细粒度识别。 粉末状物体，如药品，毒药，炸药，化妆品，食品添加剂，具有无定形，哑光，无颜色，无纹理，易与背景表面混合等特点。针对此挑战，作者提出了首个粉末物体识别的数据集以及对应的方法。 本文提出一种提高识别精度的同时，选择识别谱带的方法，显著缩短了采集时间。 这一大堆粉末名词...头晕 作者使用了三个相机(RGB,近红外NIR,短波红外SWIR)，搭建系统如下：使用了分光镜*2+反射镜*1. 使用传统的光散射和吸收理论建模，用最近邻分类器balabla，用deeplab v3+做的分割。更加依赖于物质和光的物理模型，对图像的处理很少，看起来不像是做CV的人做的。]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>光谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|Spectral Reconstruction from Dispersive Blur:A Novel Light Efficient Spectral Imager]]></title>
    <url>%2F2019%2F06%2F23%2FSpectral_from_blur_cvpr2019.html</url>
    <content type="text"><![CDATA[写在前面：此文发于CVPR2019，欢迎在文末留言交流。 题目：Spectral Reconstruction from Dispersive Blur: A Novel Light Efficient Spectral Imager作者：Yuanyuan Zhao∗,1 Xuemei Hu∗,1 Hui Guo1 Zhan Ma1 Tao Yue1,2 Xun Cao11Nanjing University, Nanjing, China2NJU institute of sensing and imaging engineering, Nanjing, China 作者提出一种利用DOB(difference of blur)获取多光谱图像的方法。在理论上证明了从单个棱镜blur图像以及一个场景中任意点光谱 恢复整幅高光谱图像的方法。 所谓DOB约束，即沿每个边缘的色散方向的色散模糊的导数正好是相邻区域的光谱差。特别地，多光谱图像重建问题可以被建模为N维线性方程组，并且通过图模型，作者理论证明了DoB约束提供了N-1个独立约束，所以需要额外的点来恢复光谱图像。作者引入边缘掩膜来获得边缘点的附加频谱信息，实现多光谱图像的全秩检索。 理论基于两个假设，1、图像可以被显示地分割为一系列区域，并且在每个区域内经过一个缩放因子光谱是可以达到均匀；2、每对相邻边沿分散方向的最大间距均大于分散尺寸。 DoB约束通过三棱镜，一个点的光谱可以分散到空间维度上。考虑两个区域$(i,j)$之间的边界，Dob可以表示为：$$\nabla_\theta b=\delta_{ij}*(s_i-s_j)\tag{1}$$ $\delta$是冲击函数，$*$是卷积，$\nabla_\theta b表示$沿着投影角度$\theta$对图像强度b求导数。因此，如果我们知道冲击函数的位置，我们就可以从色散模糊的导数中得到光谱$s_i$和$s_j$的差。 对一幅色散模糊图像，我们可以定义边界矩阵A和DoB矩阵B去表示一个DOB约束，使用AB的每一行表示一个边界的DOB约束，一幅色散模糊图所有的DOB约束可以表示为： $$AS=B \tag{2}$$ S表示光谱区域。A的每一行只有两个非零值(1,-1),下面证明A的秩为N-1，并且需要额外的光谱信息去全秩恢复S。 光谱重建图理论为了讨论A的秩，作者建立了一个相关的图模型，$\mathcal{G}=(\mathcal{V},\mathcal{\varepsilon})$,其中，V是顶点集，每一个顶点代表一个表面。$\epsilon$表示边集合。 通过引入图模型，A的每一行对应$\epsilon$的一条边。由于图像中的每一个表面至少与另一个表面相邻，因此无向图G是连通的。有以下定理： Theorem 1 The rank of the edge matrix A exactly equals to the edge number of the spanning tree of its corresponding undirected connected graph G.边矩阵A的秩恰好等于其相应的无向连通图G生成树的边数。 给出两个引理：1、连接图G和它的生成树G’有相同的谱解空间。2、无向无环图G的边矩阵A，也就是树的边矩阵A，是满行秩。 根据树的特性，有N个顶点的树G’有N-1条边，与树G’对应的边矩阵A’有N-1行，即A和A’的秩是小于等于N-1的，又根据引理2边矩阵是满秩的，所以，A和A’的秩是N-1。且A和A’的解空间是相同的。 ###Point-wise Reconstruction Algorithm如果给定分割后的图像，那么根据式(2)可以得重建每一个表面的光谱。 目标优化函数如下：$$E = E_f+\lambda_{Dob}E_{Dob}+\lambda_{side}E_{side}+\lambda_{cs}E_{cs}$$ 其中，$$E_f=|G-P_g(S)|^2+|D-P_d(S)|^2$$ 是对整体精度的约束，最基本的约束。 DoB约束:$$E_{DoB} = |\nabla_{x,y}D-P_{d}(\nabla_{x,y}S)|^2$$ 边遮挡约束： $$E_{side}=|S\odot M|^2$$M是边遮挡掩膜，$\odot $是点乘。 交叉通道与稀疏正则化： $$E_{cs}=|\nabla_{x,y}S|1+\Sigma_\lambda|\nabla{x,y}S_\lambda(x,y)-\nabla_{x,y}G|_1$$这个Loss主要是为了平滑空间域以及增强不同光谱通道上的相似的边界位置。在本文中，清晰的灰度图像可以为交叉通道与稀疏正则化提供参考图像。 后续通过引入辅助变量将此目标函数分解为两个子问题进行迭代求解，不提。]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>光谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|Skin-based identification from multispectral image data using CNNs]]></title>
    <url>%2F2019%2F06%2F21%2Fskin_MSI_cvpr2019.html</url>
    <content type="text"><![CDATA[写在前面：此文发于CVPR2019，欢迎在文末留言交流。 题目：Skin-based identification from multispectral image data using CNNs作者：Takeshi Uemori1 Atsushi Ito2 Yusuke Moriuchi2 Alexander Gatto1 Jun Murayama21Sony Europe B.V., Stuttgart, Germany 2Sony Corporation, Tokyo, Japan 作者提出一种仅利用一块皮肤的高光谱图像进行生物识别的方法，是“首个使用手掌描述了位姿不变和对遮挡鲁棒的实时人体识别系统”。 根据文献中的光学考虑，感知到的颜色主要由真皮散射、黑色素和血管吸收组成，由于皮肤生色团浓度的不同，这些颜色在个体之间是不同的。 利用手的一个Patch(16*16)，作者的CNN模型可以对注册用户进行识别，而不使用手的形状作为额外信息。此模型也可以分辨左手右手+实时。 此外，作者探究了马赛克阵列光谱相机中 光谱与空间维度的trade-off。即在固定三维体积(光谱+空间)的情况下，如何分配光谱与空间分辨率使得分类的准确性最高。然而在实际中很难找到总分辨率相同但马赛克分辨率不同的相机，所以这个探究实验是仿真的。 识别网络WideResNet作者使用了Wide-ResNet作为base network,在此网络的基础上，作者加入了spectral attention. SE-Block介绍SE-Block. SEblock就是给不同通道赋不同的权重，具体操作是对U做squeeze（global average pooling，size=W*H），得到一维向量（长度为C），然后做excitation（类似RNN中的门机制，学习参数W来建立通道相关性），得到的结果作为U中C个channel的权重做scale。其目的是学习每个通道的重要程度，增强有用的特征抑制无用的特征(Selectively enhance useful features and suppress less useful ones ) 作者提出的网络结构相比于原始的WideResNet,作者增加了SE-Block模块并且使用3D卷积替代2D卷积。网络结构没什么说的。 数据生成作者生成两个数据集，data#1,data#2.用来仿真和验证，感觉数据集的制作也是很大的一个贡献，如何设计实验验证自己的idea。 Reference【深度学习从入门到放弃】Squeeze-and-Excitation Networks)]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>光谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|Hyperspectral Image Super-Resolution with Optimized RGB Guidance]]></title>
    <url>%2F2019%2F06%2F19%2FHSI_SR_RGB_cvpr2019.html</url>
    <content type="text"><![CDATA[写在前面：此文发于CVPR2019,欢迎在文末留言交流。 题目：Hyperspectral Image Super-Resolution with Optimized RGB Guidance作者：Ying Fu1 Tao Zhang1 Yinqiang Zheng2 Debing Zhang3 Hua Huang1Beijing Institute of Technology 2National Institute of Informatics 3DeepGlint 本文提出了非监督高光谱超分辨的网络结构，并加入了CSR优化层，去选择特定的优化。 作者提出来一种端到端的RGB引导高光谱超分辨的方法，可以有效地估计RGB空间和高光谱空间的非线性映射，并且利用了空间一致性。除此之外，作者还提出CSR优化层去根据给定的CSR数据集选择最优的CSR，甚至可以设计一个有利于优化RGB引导HSI超分辨任务的新CSR函数。 建模有低分辨率HSI$X_l$，高分辨率RGB$Y$，和待恢复HSI数据$X$。因此有$$X_l=XH,~Y=CX\tag{1}$$其中$H$为空间下采样，$C$为CSR函数。 现有的从高分辨率RGB和低分辨率HSI回复高分辨率HSI可以表示为$$E(X)=E_d(X,X_l,Y)+\lambda E_s(X) \tag{2}$$ $E_d$表示为根据式(1)约束的恢复$X$，$E_s(\cdot)$为先验正则，之前的工作均假定有限的材料类型从而在稀疏假设下可以线性表示。非线性光谱表达可以显著地改善高光谱恢复的精度。 RGB引导光谱CNN网络结构如图1 所示。 首先使用式(1)对光谱图像进行光谱维度下采样，得到低分辨率的RGB图像，后使用CNN学习低分辨率RGB到低分辨率光谱图像之间的非线性映射，与此同时，RGB加到每层的特征上用于引导空间分辨率重建。有个疑问：既然$Y_l=CX_l$,降质过程（$C$）既然已知，为什么还要去学这个逆问题呢？这里为了学习到光谱信息的内在特征，使用了1*1的卷积核。Loss为MSEloss。 $$\mathcal{L}_s=|f(Y_l,\Theta)-X_l|^2\tag{3}$$由于网络参数对于高分辨率RGB是权值共享的，所以$$X=f(Y,\Theta)\X_l=f(Y_l,\Theta)\tag{4}$$ 又根据式(1)，高分辨率HSI 与 经过线性映射的低分辨率HSI和高分辨率RGB是具有一致性的。所以，联合式(1)(4),可以得到$$\mathcal{L}_d=|Y-Cf(Y,\Theta)|^2+\tau_1|X_l-f(Y,\Theta)H|^2\tag{5}$$ 最终的loss为： $$\mathcal{L}_sd = \mathcal{L}_d(\Theta)+\tau_2 \mathcal{L}_d(\Theta)+\eta_1|\Theta|^2$$ 图1 CSR优化之前的研究解释了CSR对于HSI的恢复具有显著影响。 随后，作者提出了两种获取优化的CSR的方法。对于已知的RGB相机，作者设计了一个卷积层用于选择优化的CSR。除了CSR选择，CSR也可以在一定的物理条件约束下通过卷积层仿真。即，可以select or design .由于本文的网络是非监督的，即理论上讲对于每一幅图像都可以得到一个新的CSR，但是这种方式对于同一个RGB相机意义不大。所以，对于一个RGB相机，只需要在一个数据集上优化得到CSR即可。 作者使用j个CSR函数对t个HSI生成RGB图像，得到$\mathcal{Y}t = (Y{1,t},Y_{2,t},\cdots,Y_{j-1,t},Y_{j,t})$，一副图像对应一个CSR函数，所以，选图像=选CSR。 图2(a)展示了CSR选择层的结构，表达成公式形式为式(6).本质上是对所有的CSR函数做了一组加权平均，权值是可学的卷积核。从基的角度理解，卷积核学习到的是CSR基的系数。【看文章描述这里后面还做了一个argmax()完成“选择”，而不是加权平均】 $$\hat{Y_t}=stack(V\mathcal{Y_t}(R)),V\mathcal{Y_t}(G)),V\mathcal{Y_t}(B))\tag{6}$$上式中$V$为 11的卷积。 图2 下面又对CSR的非负性做了约束得到损失函数$\mathcal{L}_{cs}$ 【这种方式和穷举所有csr算MSE，从所有的CSR中找出最接近的CSR有什么区别吗？只是用卷积形式表示出来？】 更进一步如图2(b)，作者提出可以用一组卷积核直接学习到CSR函数，并加入了平滑和非负约束得到$\mathcal{L}_{co}$。【其实上一步就自然而然想到了CSR系数了啊】 最终对应两种CSR层的设计方式(select or design)，就有对应的两种loss。$$\mathcal{L} = \mathcal{L}{cs}(V) + \tau_3\mathcal{L}{sd}(\Theta).\\mathcal{L} = \mathcal{L}{co}(V) + \tau_4\mathcal{L}{sd}(\Theta).\$$总结：这篇文章设计网络部分疑问较多。 Reference]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>光谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|Multispectral and Hyperspectral Image Fusion by MS/HS Fusion Net]]></title>
    <url>%2F2019%2F06%2F19%2FMS_HS_fusion_cvpr2019.html</url>
    <content type="text"><![CDATA[写在前面：此文发于CVPR2019，欢迎在文末留言交流。 题目：Multispectral and Hyperspectral Image Fusion by MS/HS Fusion Net作者：Qi Xie1, Minghao Zhou1, Qian Zhao1, Deyu Meng1,∗ , Wangmeng Zuo2, Zongben Xu1Xi’an Jiaotong University; 2Harbin Institute of Technology 作者提出了一种融合HR-MS和LR-HS的方法，从而生成HR-HS。作者提出的想融合模型考虑了低分辨率图像的观测模型和HR-HS的低秩特性。随后作者设计了一种迭代优化方法，并利用近端梯度法(Proximal Gradient Method, PG)求解此模型。最后作者将以上模型表示成深度网络形式(MS/HS Fusion Net)，通过卷积神经网络学习近端(proximal)操作子和模型参数。 直接恢复高分辨率的高光谱图像是一个病态的逆问题，通常操作是赋予先验。如，1、利用HR-MS训练的字典可以稀疏表示HR-HS的空间信息；2、假设HR-HS的局部空间平滑先验，使用全变分正则将其编码到优化模型中；3、除了探索空间先验，充分利用内部光谱相关性作为先验，使用低秩方法在光谱维度上编码先验以减少光谱畸变。 本文的贡献如下：1本文提出的融合模型不仅考虑了观测模型，并且使用了近似低秩先验结构以减少光谱畸变。2、将迭代策略集成到深度网络框架中。3、实验证明结果有效。 基于机器学习的方法1、使用稀疏编码在HR Patch上学习字典，然后学习从LR到HR的相关系数矩阵。2、稀疏矩阵分解学习LR光谱字典然后使用光谱字典和HRMS图像共同重建HRMS。3、非负矩阵分解。 建模基本模型：$$Y=XR+N_y\tag{1}$$$$Z=CX+N_y\tag{2}$$ 其中$Y$是HRMS，$X$是HRHS，$R$为MS的光谱响应曲线，$N_y$是HRMS的噪声。$Z$是LRHS，$C$是线性下采样操作，$N_z$是LRHS的噪声。 理论1： 又由于$\tilde{Y}=Y-N_y$,所以式(1)可等价于$$X=YA+\hat{Y}B+N_x \tag{5}$$其中，$Y$秩为s，$X$秩为r，$[Y,\hat{Y}]$秩为r。 同时存在以下结论： 所以式(1)(2)可以表示成$$Z=C(YA+\hat{Y}B)+N \tag{8}$$ 这里Z的秩为r. 【一个疑问，这里的秩只针对的是光谱维度？理论上讲，lr相比较于HR也是降秩了的。但这里Z好像和X的秩是一样的？这种一样是因为$Z=CX$中C是线性下采样？】 作者解释这种优化可以保持空间信息。 我自己来看，恢复HRHS可以从两个方向去看，一个是从HRMS进行光谱维度上采样，LRHS提供光谱维度的引导信息；另一个是从LRHS进行空间维度上采样，HRMS提供空间维度的引导信息。两种方式孰优孰劣，直觉上空间维度的信息其实是更难恢复的。 近端梯度法（Proximal Gradient Method ，PG）近端梯度法是一种特殊的梯度下降方法，主要用于求解目标函数不可微的最优化问题。如果目标函数在某些点是不可微的，那么该点的梯度无法求解，传统的梯度下降法也就无法使用。PG算法的思想是，使用临近算子作为近似梯度，进行梯度下降。 网络模型 Reference近端梯度法(Proximal Gradient Method, PG)]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>光谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior]]></title>
    <url>%2F2019%2F06%2F18%2FHSI_deep_spatial_spectral_prior.html</url>
    <content type="text"><![CDATA[写在前面：此文发于CVPR2019，欢迎在文末留言讨论。 题目：Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior作者：Lizhi Wang1 Chen Sun1 Ying Fu1 Min H. Kim2 Hua Huang11Beijing Institute of Technology 2Korea Advanced Institute of Science and Technology 正则化是求解病态优化问题的一个基本方法，并且在高光谱图像重建中得到了广泛应用。但是以往基于正则化的方法往往需要手工设计且对变化范围大的场景不鲁棒。作者提出了一种结合数据驱动先验和基于优化网络的高光谱图像重建方案，在仿真和实物系统上达到了较高精度。 为了解决欠定重建问题，可以使用正则化引入图像先验，如全变分(TV)，sparsity,non-local similarity等正则化方法。但基于经验设计的正则化方法对多样的自然光谱图像处理能力差，需要手工微调参数且往往不能求取闭合解。近期提出的基于神经网络的压缩感知方法，LISTA,ADMM-NET,ISTA-NET等虽然可以避免迭代求解，但是仍然继承了稀疏先验这一特点，限制了一些层内的特征是稀疏的，这对于求解优化问题是不利的。同时，现有基于网络的重建方法只考虑了空间维度而忽略了光谱图维度，这相当于白白损失了一个维度的先验。 作者结合优化的内在结构(structure insight of the optimization)和神经网络出色的先验建模能力，1、首先学习先验的 正则化描述子。2、将描述子与优化方法结合表示成网络形式。 HQS半二次方分裂首先需要介绍HQS算法： 有目标函数如下，其中$x$为待恢复图像。$$\hat{x}=\arg\min_x\frac{1}{2}|y-Hx|+\lambda\Phi(x)$$ 引入辅助变量$z$,$$\hat{x}=\arg\min_x\frac{1}{2}|y-Hx|^2+\lambda\Phi(z),s.t.~~z=x$$ 于是得到惩罚函数： $$\mathcal{L_\mu}(x,z)=\frac{1}{2}|y-Hx|+\lambda\Phi(z)+\frac{\mu}{2}|z-x|^2$$ 于是问题转化为迭代求解以下两式： $$x_{k+1}=\arg\min_x|y-Hx|+\mu|x-z_k|^2$$ $$z_{k+1} = \arg\min_z\frac{\mu}{2}|z-x_{k+1}|^2+\lambda\Phi(z)$$ 对应于本文要解决的压缩感知问题，当正则项不可微时，使用分离变量方法把正则项解耦出来，本文使用了HQS方法。如图1所示，对应的网络模型 图1 从公式(7)(8)的角度考虑，可HQS方法分离了观测模型$\Phi$和超参先验$R(\cdot)$，所以可以使用卷积神经网络学习一个$S(\cdot)$来代替公式(8)的求解，如图2公式： 图2 自然而然，一个卷积神经网络可以代替图2，网络结构如图3所示。 图3 此网络的优点在于：1、考虑了空间维度和光谱维度；2、简单易学好训练。 优化与重建结合的方法与分离变量并进行迭代的方式相比，作者提出的方法整体考虑了观测模型和图像先验。图1公式(7)中的f-子问题是一个最小二成问题，可以给出闭合解： 利用共轭梯度法(conjugate gradient, CG)可以求解f-子问题，易推出(11)(12), 将先验生成网络嵌入到(12)，得到下图的网络。 最终可以端到端训练进行重建。取得了STOA的结果。 Reference机器学习&amp;数据挖掘笔记_12（对Conjugate Gradient 优化的简单理解） HQS——Half Quadratic Splitting半二次方分裂 Learning Deep CNN Denoiser Prior for Image Restoration]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>光谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记|Valse2019]]></title>
    <url>%2F2019%2F04%2F15%2FValse2019.html</url>
    <content type="text"><![CDATA[OverviewValse 2019 本次valse注册会议的人数达到了5000+，现场也异常火爆，许多workshop/tutorial开场前10min会场就已经坐满。而第一天的三维与深度学习的workshop则因为场地太小，主办方在已经开讲的情况下转移会场。这场汇集了华人CV青年学者的研讨会是当前深度学习与CV领域的一个缩影，异常火爆与拥挤，但也百花齐放，成果丰硕，不断碰撞出新的花火。 本届valse主题非常丰富，几乎包含了计算机视觉领域的所有热点，以弱监督、迁移学习、元学习、模型与网络设计、医学影像、三维重建为代表的主题是学界关注的重点。从公司workshop和展区来看，当前工业界并不怎么关注弱监督、迁移学习等概念，业界更关注CV与AI的直接产业化，如轻量级网络的设计，网络模型搜索，行人识别、语义分析、三维场景地图等与产品更近的技术）。 会议的很多workshop是同步进行的，所以有必要根据自己的兴趣主动做出取舍。当然，有时是因为会场爆满“被动”去听另一个完全听不懂的workshop。印象较深与略有收获主要集中在第一天与最后一天：第一天workshop“三维视觉与深度学习”，最后一日“深度学习模型设计”和poster环节。其他诸如“meta-learning”“弱监督语义分割”“single net work do more without retraining”等，我没有相关基础或因讲者进度过快推介自己的工作，听下来如蜻蜓点水，收获不大。 首先说下三维视觉的workshop。三维视觉是计算机视觉中最基础也是最经典的问题。因为转场等原因，我是从第二位国防科大的徐凯老师开始听的（后来听说第一个百度的一直在打广告，没听也不是太遗憾）。徐所讲的是通过hierarchial信息进行三维重建，介绍的工作很多，时间跨度较大，从2011年的工作一直到最近的工作，核心的思路是通过图模型对3D图像进行建模，关键是如何使用合适的结点表述以及连接关系对三维图像进行结构描述，徐在此基础上完成了从二维图像和恢复三维物体，并使用auto-encoder等结构完成对三维信息的重建、理解（分割）和生成。 第三位是清华的刘烨斌老师，主题是三维人体重建。关于人体的重建更多地是应用于娱乐方面，例如VR，影视、游戏、广告等（这时相关的炫酷应用吸引了观众的眼球）。人体动态重建的目标是能够达到大规模便捷实时的精准采集与重建。刘老师科普了这方面工作的发展：1、基于多视立体几何的方法具有重建质量高、支持任意拓扑等优点，但其实时性较差；2、基于三维模板的方法可以降低求解时间；3、基于统计模板的方法无任何人工预处理同时具有语义信息，但难以重建复杂的几何拓扑；4、而基于表面动态融合的方法无需人工预处理，甚至仅需单视点，但是以来实时深度相机或者深度计算性能，对于快速运动的人体效果差。随后介绍了他们的工作：单深度相机融合结合体态模板实时重建（double fusion，2+4），效果鲁棒性好，支持复杂拓扑结构和复杂纹理。（据我了解后来在double fusion的基础上将单深度相机扩展为多深度相机…）介绍的第二个工作是单深度相机语义化动态三维重建（MulayCap），视频展示效果仍旧酷炫。。。最后刘老师给出了未来三维人体重建领域的展望（如下ppt）。 下一位是自动化所模式国重的申抒含老师，报告更加科普。把三维场景重建的流程附加代表性结果讲了一遍，包括稀疏重建、稠密重建、语义建模、矢量建模等，最后介绍了几个应用。重建效果好的可怕。 最后一位是港科大的沈劭劼老师，最初在会议手册看到这个HKUT和报告题目“…drones”就猜测是大疆做的，ppt出来后果不其然，论文中有Dji的参与。内容是介绍利用无人机进行三维场景重建，更贴近SLAM。本来slam创新研究的门槛就已经较高了，这种结合无人机的工作更加困难，没有get到点。 最后一日poster环节主要和曹讯老师的“hyperspectral imaging with random printed mask(cvpr2019)”的作者进行了讨论（讲解的是二作）。这篇文章的主要想法是通过在普通rgb相机前增加一个随机打印的彩色掩模来复原rgb图像，也是通过神经网络来恢复31个band的高光谱图像。相比于直接从rgb图像恢复高光谱，这种增加了随机色彩的掩模的成像系统具有更加准确的光谱精度。可以理解为增加了光谱基，使得恢复的高光谱图像精度更高。这个掩模是采用普通彩色打印机打印的，打印的图案为随机的彩色墨点，通过不同颜色墨点的混合，可以得到更多的色彩。个人感觉这是对于掩模式光谱相机的改进，通过与讲者的交流，在实现过程中有很多细节，实现这个系统还是要经过相当的积累与工作的。 这篇文章作者的另一篇工作“spectral reconstruction from dispersive blur: approaching full light throughput spectral imager”，通过分光镜获取灰度图像和因光路导致边缘blur的光谱图像，进行光谱重建。这篇文章通过图模型证明了实验可行性，没有看懂，且这篇文章和光学联系更加紧密。 总结与感受： 1、poster现场来看，tracking和reid独占两排，来势汹汹。基于single Image的xx 的poster也较多. 2、弱监督也需增加关注（两个弱监督workshop） 3、强化学习并没有想象中的火热，只有极少数的文章提到了强化，强化学习的威力还没有展现出来。 4、更少的信息输入（single image/view），更高维度的输出（3D）。三维相关领域是个可选方向，但是需要长时间的耕耘。 5、了解了新概念meta-learning，感觉还在算法起步阶段。 6、图模型和神经网络结合可能是未来图像相关算法的发展方向 7、自己仍需努力。]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>XJBX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux运行OpenGL]]></title>
    <url>%2F2018%2F11%2F24%2Fopengl1.html</url>
    <content type="text"><![CDATA[写在前面：在linux下开发OpenGL。简单介绍OpenGL并安装，编写第一个程序。 环境：Ubuntu 16.04 LTSGraphics:GT 730gcc 5.4.0 什么是OpenGLOpenGL 是一套由SGI公司发展出来的绘图函式库，它是一组 C 语言的函式，用于 2D 与 3D 图形应用程式的开发上。OpenGL 让程式开发人员不需要考虑到各种显示卡底层运作是否相同的问题，硬体由 OpenGL 核心去沟通，因此只要显示卡支援 OpenGL，那么程式就不需要重新再移植，而程式开发人员也不需要重新学习一组函式库来移植程式。 安装1.首先安装编译器与基本函数库，一般情况下系统均已安装： 1$ sudo apt-get install build-essential 2.其次安装 OpenGL Library 1$ sudo apt-get install libgl1-mesa-dev 3.安装OpenGL Utilities 1$ sudo apt-get install libglu1-mesa-dev OpenGL Utilities 是一组建构于 OpenGL Library 之上的工具组，提供许多很方便的函式，使 OpenGL 更强大且更容易使用。 4.安装OpenGL Utility Toolkit 1$ sudo apt-get install libglut-dev OpenGL Utility Toolkit 是建立在 OpenGL Utilities 上面的工具箱，除了强化了 OpenGL Utilities 的不足之外，也增加了 OpenGL 对于视窗介面支援。 注意：在这一步的时候，可能会出现以下情况，shell提示： 1234Reading package lists... DoneBuilding dependency treeReading state information... DoneE: Unable to locate package libglut-dev 将上述$ sudo apt-get install libglut-dev命令改成$ sudo apt-get install freeglut3-dev即可。 第一个OpenGL程序代码： 123456789101112131415161718192021222324252627282930313233343536#include &lt;GL/glut.h&gt;void init(void)&#123; glClearColor(0.0, 0.0, 0.0, 0.0); glMatrixMode(GL_PROJECTION); glOrtho(-5, 5, -5, 5, 5, 15); glMatrixMode(GL_MODELVIEW); gluLookAt(0, 0, 10, 0, 0, 0, 0, 1, 0); return;&#125;void display(void)&#123; glClear(GL_COLOR_BUFFER_BIT); glColor3f(1.0, 0, 0); glutWireTeapot(3); glFlush(); return;&#125;int main(int argc, char *argv[])&#123; glutInit(&amp;argc, argv); glutInitDisplayMode(GLUT_RGB | GLUT_SINGLE); glutInitWindowPosition(0, 0); glutInitWindowSize(300, 300); glutCreateWindow(&quot;OpenGL 3D View&quot;); init(); glutDisplayFunc(display); glutMainLoop(); return 0;&#125; 编译程序： 1$ gcc -o test test.c -lGL -lGLU -lglut 执行： 1$ ./test 结果： 期间编译时报错： 12345678910111213141516171819test.c: In function ‘main’:test.c:30:5: error: stray ‘\342’ in program glutCreateWindow(“OpenGL 3D View”); ^test.c:30:5: error: stray ‘\200’ in programtest.c:30:5: error: stray ‘\234’ in programtest.c:30:32: error: invalid suffix &quot;D&quot; on integer constant glutCreateWindow(“OpenGL 3D View”); ^test.c:30:25: error: ‘OpenGL’ undeclared (first use in this function) glutCreateWindow(“OpenGL 3D View”); ^test.c:30:25: note: each undeclared identifier is reported only once for each function it appears intest.c:30:32: error: expected ‘)’ before numeric constant glutCreateWindow(“OpenGL 3D View”); ^test.c:30:32: error: stray ‘\342’ in programtest.c:30:32: error: stray ‘\200’ in programtest.c:30:32: error: stray ‘\235’ in program 解决方法：把glutCreateWindow(&quot;OpenGL 3D View&quot;);中的中文“”换成英文&quot;&quot;。参考：Stray ‘\342’ in C++ program Reference本文内容主要参考Ubuntu 安装OpenGLUbuntu14.04下配置OpenGL及测试代码]]></content>
      <categories>
        <category>OpenGL</category>
      </categories>
      <tags>
        <tag>OpenGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CUDA程序开发]]></title>
    <url>%2F2018%2F11%2F16%2FCUDA.html</url>
    <content type="text"><![CDATA[写在前面：GPU并行计算和CUDA程序开发及优化 课堂笔记+课程作业主要是一些小程序练手 GPU相关知识GPU的计算模式在异构协同处理计算模型中将CPU与GPU结合起来加以利用。应用程序的串行部分在CPU上运行，而计算任务繁重的部分则由GPU的高性能计算来进行。从用户的角度来看，应用程序只是运行得更快了，获得了很好的性能提升。 高性能计算机的 分类单指令流单数据流（ SISD）• 单指令流多数据流（ SIMD）• 多指令流单数据流（ MISD）• 多指令流多数据流（ MIMD） 单独的高性能计算节点主要分为：• 同构节点（仅采用CPU， Intel Xeon CPU、 AMD Opteron CPU）• 异构节点（分为主机端和设备端，分别注重逻辑运算和浮点计算。 主流异构节点类型包括CPU+GPU和CPU+MICMIC： Many Integrated Core （Intel 集成众核） GPU硬件架构NVIDIA不同架构产品不同GPU架构的设计理念、工艺水平等均不相同，相应的内部体系结构和性能也不相同。每一构架都对应大量产品 Tesla Fermi Kepler Maxwell Pascal Volta GPU体系结构相关术语 SP（ Streaming Processor） :流处理器是GPU运算的最基本计算单元。 SFU（ Special Function Unit） :特殊函数单元用来执行超越函数指令，比如正弦、余弦、平方根等函数。 Shader core（渲染核/着色器）， SP的另一个名称，又称为CUDA core，始于Fermi架构 DP （双精度浮点运算单元） SM（ Streaming Multiprocessors） :流式多处理器是GPU架构中的基本计算单元，也是GPU性能的源泉，由 SP、DP、 SFU等运算单元组成。这是一个典型的阵列机，其执行方式为SIMT（单指令多线程），区别于传统的 SIMD（单指令流多数据流），能够保证多线程的同时执行 SPA（ Scalable streaming Processor Array）可扩展的流处理器阵列：所有处理核心和高速缓存的总和，包含所有的SM、 TPC、 GPC。与存储器系统共同组成GPU构架 MMC（ MeMory Controller）存储控制器：控制存储访问的单元，合并访存。每个存储控制器可以支持一定位宽的数据合并访存。 ROP（ raster operation processors）光栅操作单元 LD/ST（ Load/Store Unit）存储单元 ![deviceQuery](./2018-11-16-CUDA/deviceQuery.png) deviceQuery 从图中可以看出，我的GPU为GT730，CUDA版本为9.0，GPU显存为2G，拥有384个CUDA核，GPU最大时钟频率为0.9Ghz，显存带宽为64位，每个block最多支持1024个线程。 与1080TI/k20相比，有些参数没差多少。 ReferenceCSDN解读 RCAN Image Super-Resolution Using Very Deep Residual Channel Attention Networks-ECCV2018]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|Unsupervised Monocular Depth Estimation with Left-Right Consistency]]></title>
    <url>%2F2018%2F11%2F07%2Fmono_depth.html</url>
    <content type="text"><![CDATA[写在前面：此文发于CVPR2017，并在项目主页公布了代码。本文中文翻译：读Unsupervised Monocular Depth Estimation with LeftRight Consistency代码中文解读：[读源码] Unsupervised Monocular Depth Estimation with Left-Right Consistency关于近几年单目深度估计的文章，可以参考知乎用户的回答。 题目：Unsupervised Monocular Depth Estimation with Left-Right Consistency作者：Clement Godard Oisin Mac Aodha Gabriel J. BrostowUniversity College London 背景motivation思考：人眼可以做到单目的原因是什么？ 回答：人眼单目深度估计是基于极强的先验，这也限制了单目深度估计的应用场景。 “利用图像重建误差（image reconstruction loss）来最小化光度误差（类似于SLAM中的直接法）虽可以得到很好地图像重建结果（disparity），但得到深度预测结果非常差。”（不太了解SLAM直接法是什么） 本文主要讲无监督学习的方式估计深度。基本思想基于立体匹配中的左右一致性，即将一幅图warp到另一幅图定义loss。 保留问题： 单目无监督深度的开山作是Depth Map Prediction from a Single Image using a Multi-Scale Deep Network（NIPS2014），第一篇CNN-based来做单目深度估计的文章。 other works Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural network(ECCV2016),J. Xie, R. Girshick, and A. Farhadi.计算出每个像素的视差的概率分布，对像素所在行的所有视差进行加权求和，权重即为概率。 Unsupervised CNN for single view depth estimation: Geometry to the rescue.(ECCV2016),R. Garg, V. Kumar BG, and I. Reid. 贡献 新的loss，end2end的unsupervised monocular depth estimation网络 应用在了新的数据集上 对train loss和图像生成模型进行了评估。图像生成模型是指生成视差图的模型。 主要内容网络结构： encoder-decoder结构，包括长skip。通过一个带scale的sigmoid函数，将输出限制在(0,0.3*当前scale下的图像宽度)。作者采用的激活为ELU（参看深度学习(1)-深度学习中的核函数（激活函数））。decoder结构中，作者没有用广泛采用的deconvolution结构，而是用一个最近邻升采样+后续的卷积层的方式来做分辨率提升。作者采用Adam模式（参见cs31n_lesson6_7/5.调参）来进行训练。 和一般视觉深度学习训练一样，数据增强（data augmentation）是必要的。 作者也尝试将encoder替换成Resnet50，而其他部分不变。 LOSS： $$ C_s=\alpha_{ap}(C^l_{ap}+C^r_{ap})+\alpha_{ds}(C^l_{ds}+C^r_{ds})+\alpha_{lr}(C^l_{lr}+C^r_{lr}) $$ $C_{ap}$:纹理loss，与输入图像纹理相关。$\alpha=0.8$$C_{ap}$来自于Loss Functions for Neural Networks for Image Processing,也是主要起作用的部分！ $C_{ds}$:平滑loss直观理解，原图I梯度越大的地方，要求视差图d梯度越大；原图I梯度越小的地方（平滑），允许视差图d梯度越小（平滑）。 $C_{lr}$:左右一致性loss 速度35 ms for a 512*256 image on one modern GPU 吐槽【作者在估计深度的过程中确实没有用到激光或者结构光的GT，但仍需要已知基线和焦距的双目图像对作为训练输入，测试时只需将单张图作为输入。感觉monocular有点牵强】 补充阅读 Dispnet &amp; FCN，本文使用的网络结构 STN ，作者warp图像的主要方式 本文提到另一个工作DeepStereo：从多个邻近视角选取像素合成新视角的图像。Deepstereo: Learning to predict new views from the world’s imagery. (CVPR2016)J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Reference论文笔记-深度估计(5)Unsupervised Monocular Depth Estimation with Left-Right Consistency]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>深度信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记|DEEP3D]]></title>
    <url>%2F2018%2F11%2F07%2FDEEP3D.html</url>
    <content type="text"><![CDATA[写在前面：Deep3d,由2D到3D视频转换，非常经典的视角重建。有码。论文：Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks，作者：Junyuan Xie, Ross Girshick, Ali FarhadiUniversity of Washington 所以我们的方法并不是先预测一张深度图，然后用这张深度图通过一个单独的算法去重建缺失的视角，而是在同一神经网络中重新创建端到端的方法来训练它。 ReferenceDeep3D 中文翻译及阅读笔记开源|如何使用CNN将视频从2D到3D进行自动转换（附源代码）]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>深度估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：SR总结]]></title>
    <url>%2F2018%2F11%2F07%2FSR.html</url>
    <content type="text"><![CDATA[写在前面：这是一篇不太完善的CNN超分辨总结，整理了近年来深度学习在超分辨领域比较有代表性的工作，随缘更新。 SRCNN (有码) 论文： Learning a Deep Convolutional Network for Image Super-Resolution, ECCV2014 Image Super-Resolution Using Deep Convolutional Networks, TPAMI2015 项目主页：http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html作者：Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. VDSR 论文：Accurate Image Super-Resolution Using Very Deep Convolutional Networks, CVPR2016代码： official: https://cv.snu.ac.kr/research/VDSR/ pytorch: https://github.com/twtygqyy/pytorch-vdsr tensorflow: https://github.com/Jongchan/tensorflow-vdsr 作者：Jiwon Kim Jung Kwon Lee Kyoung Mu Lee ESPCN论文：Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network，CVPR2016代码：https://github.com/Tetrachrome/subpixel作者：Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken,Rob Bishop, Daniel Rueckert, Zehan Wang SRGAN 论文：Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, CVPR2017代码： github(tensorflow): https://github.com/zsdonghao/SRGAN github(tensorflow): https://github.com/buriburisuri/SRGAN github(torch): https://github.com/junhocho/SRGAN github(caffe): https://github.com/ShenghaiRong/caffe_srgan github(tensorflow): https://github.com/brade31919/SRGAN-tensorflow github(keras): https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks github(pytorch): https://github.com/ai-tor/PyTorch-SRGAN作者：Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham,Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi（Twitter的十一个作者，和deepmind的二十几个作者还有差距:) ） RCAN论文：RCAN Image Super-Resolution Using Very Deep Residual Channel Attention Networks-ECCV2018代码：github(official-pytorch)https://github.com/yulunzhang/RCAN作者：Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu ReferenceGAN相关：SRGAN，GAN在超分辨率中的应用从SRCNN到EDSR，总结深度学习端到端超分辨率方法发展历程A collection of high-impact and state-of-the-art SR methodsSuper-Resolution via Deep LearningCSDN解读 RCAN Image Super-Resolution Using Very Deep Residual Channel Attention Networks-ECCV2018]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>超分辨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：Zoom and Learn:Generalizing Deep Stereo Matching to Novel Domains]]></title>
    <url>%2F2018%2F10%2F22%2Fzoom_and_learn_stereo_domin.html</url>
    <content type="text"><![CDATA[写在前面：CVPR2018，针对双目匹配在训练和实际应用中存在的domin transfer问题。由于商业原因，作者只公布了测试代码。 ![phone](paper_Zoom and Learn/phone.png) 题目：Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains作者：Jiahao Pang$^1$ Wenxiu Sun$^1$ ChengxiYang$^1$ JimmyRen$^1$ RuichaoXiao$^1$ JinZeng$^1$ LiangLin$^{1,2}$$^1$SenseTime Research $^2$Sun Yat-sen University 背景motivation立体匹配通用难点：基于CNN的立体匹配的一个难点在于训练得到的立体匹配模型往往不能使用在真实场景(new domain)中，即两个domain之间存在gap(这也是CNN的一个通病)。 两个常识： 预训练好的模型在新domain直接使用会在边界出现artifacts 使用up-sampled的图像对生成disparity会带来额外的细节 基于这两个常识，作者使用迭代优化的方式在 让CNN得到高空间分辨率的输出(常识2) 的同时，利用graph Laplacian regularization保留边界且平滑边界的artifacts(常识1). 作者在手机拍摄的日常场景和KITTI上进行了实验验证。 other works监督： DispNet,first end-to-end CNN stereo matching CRL:A two-stage convolutional neural network for stereo matching. (ICCVW2017) GC-NET:End-to-end learning of geometry and context for deep stereo regression.（ICCV2017） DRR:Detect, replace, refine: Deep structured prediction for pixel wise labeling.(CVPR2017) 半监督： Unsupervised monocular depth estimation with left-right consistency(CVPR2017) Semi-supervised deep learning for monocular depth map prediction.(CVPR2017) Self-supervised learning for stereo matching with self-improving ability.arXiv Unsupervised learning of stereo matching.(ICCV2017)‘ 目前常用方式，合成数据集上训练后再在有GT的目标数据上finetune. End-to-end learning of geometry and context for deep stereo regression.（ICCV2017） A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation.(CVPR2016) A two-stage convolutional neural network for stereo matching.(ICCVW2017) Graph Laplacian regularization. 这里引用集中在denosing方面，不看了。 Iterative regularization/filtering 引用为image restoration方面，作者将其引入到CNN。 作者还专门强调，不同于堆叠CNN，这里的iteration是在训练过程中的iteration. 主要内容尺度多样性。 算法 ![Algorithm](paper_Zoom and Learn/Algorithm.png) 评价指标定量评价这里的评价指标很有意思。由于作者在三个数据集上进行了验证，有一个是和训练集相同的FlyingThings3D-80,另外一个是没有GT的手机拍摄的数据集，还有KITTI2015。 ![performace1](paper_Zoom and Learn/performance1.png) ![performace2](paper_Zoom and Learn/performance2.png) EPE:End-point-End3ER:three-pixel error rate这里所谓的PSNR和SSIM是利用disparity和右图生成的左图与真正的左图计算的PSNR和SSIM。左右图warp后肯定会有occlusion的存在，不知道这么PSNR意义是否很大。 视觉指标 ![result1](paper_Zoom and Learn/result1.png) ![result2](paper_Zoom and Learn/result2.png) 总结总结起来，使用了graph Laplacian 保持物体的边界，具体方法迭代patch。大概就这样。与另一篇相似的工作Unsupervised Adaptation for Deep Stereo相比，指标高了那么一diudiu。 相关工作Unsupervised Adaptation for Deep Stereo代码：tensorflow版本、caffe版本 文章有何贡献：本文提出了一种新的 fine-tuning 的方法使在大量合成数据上训练的 DispNet 可以迁移到无 groudtruth 或者只有极少量的 groundtruth 的实际数据集上。 本文研究的问题有何价值：双目深度估计的标签现实中很难获得，本文提出的 fine-tuning 方法可以在没 有groundtruth 的情况下将模型迁移过来。 所研究问题有何困难：如何获得可靠的监督信息来 fine-tune。 本文的解决思路是怎样的：文章受在 Kitti 数据集上 fine-tune 的启发，发现利用稀疏的标签也可以很好地对模型进行训练。文章利用传统算法如 AD-CENSUS 或 SGM 生成 label 来作为 groundtruth， 同时利用 CCCN（一种 confidence measure 的方法）来选取可信度高的 label，只利用这部分置信度高的 sparse label 来 fine-tune。 Reference本期最新 9 篇论文，每一篇都想推荐给你 | PaperDaily #14]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>深度信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：Pyramid Stereo Matching Network]]></title>
    <url>%2F2018%2F10%2F18%2FPSM_Net.html</url>
    <content type="text"><![CDATA[[//]写在前面：此文发于CVPR2018，并公布了代码 题目：Pyramid Stereo Matching Network作者：Jia-Ren Chang, Yong-Sheng ChenDepartment of Computer Science, National Chiao Tung University, Taiwan 背景motivation立体匹配通用难点：occlusion areas,repeated patterns,textureless regions,reflective surfaces.本文主攻方向：单独使用intensity-consistency导致在纹理区域表现不佳，现有立体匹配框架多基于patch-based Siamese结构且缺少context信息。从全局context信息中提取regional support. other works MC-CNN Displets: utilizes object information by modeling 3D vehicles to resolve ambiguities in stereo matching. ResMatchNet: learns to measure reflective confidence for the disparity maps to improve performance in ill-posed regions. GC-Nets: employs the encoder-decoder architecture to merge multiscale features for cost volume regularization. 贡献 提出一个无后处理的end2end的立体匹配学习框架 引入pyramid pooling模块使提取的图像特征包含全局context信息 提出 堆栈沙漏式的3D-CNN ，去扩展cost volume时的局部的上下文信息 在KITTI刷到了state-of-the-art（目前被M2S_CSPN等挤下去了，在stereo2012/2015分别排名8/20 截止2018.10.18） 主要内容提出PSM-Net，包括两个主要模块，spatital pyramid pooling 和3D-CNN.spatial pyramind pooling 在不同尺度和位置聚合了全局context信息，并形成cost volume3D-CNN使用stacked mutiple hourglass结构(来源：Stacked Hourglass Networks for Human Pose Estimation,ECCV2016)标准化cost volume并链接中间的监督。 网络结构 相关工作本文主要借鉴了两种提取全局上下文信息的网络结构：hourglass 和 pyramid pooling.两种结构均有大量相关的工作。 ReferencePyramid Stereo Matching Network]]></content>
      <categories>
        <category>文章阅读</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>深度信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The volume boot has only 5.1MB disk space remaining]]></title>
    <url>%2F2018%2F08%2F16%2Fboot-space.html</url>
    <content type="text"><![CDATA[/boot 目录中是系统引导文件和内核，更新内核之后旧内核还存放在里面，安装软件的时候就会提示 /boot 空间不足，佳解的决办法就是将旧内核删除。 1、查看磁盘使用情况 df -h 2、查看正在使用的内核版本 uname -auname -r都可以 3、查看系统中存在的内核版本 dpkg –get-selections | grep linux（当出现deibstall,则说明已经删除） 4、把低于当前版本的内核删掉 sudo apt-get remove linux-image-4.13..0-41-genericsudo apt-get autoremove linux-image-4.13..0-41-generic都可以，至于autoremove 和remove的区别，我先用remove再用autoremove实验了一下，发现remove只能删除指定的image，而autoremove则可以把这个内核相关的“linux-headers-4.xx.x-xx”等东西删掉，删的更加彻底。 5、删完之后df -h]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>XJBX</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则项之L0L1L2]]></title>
    <url>%2F2018%2F08%2F03%2Fl0l1l2re.html</url>
    <content type="text"><![CDATA[注意，本文讨论的是L0L1L2范数作为代价函数/损失函数/目标函数/期望风险中正则项/结构风险的应用，而不是作为经验风险项的应用。 在经验风险中，L1(MAE)L2(MSE)两者常被用来约束数据的逻辑回归。虽然同样用于逻辑回归，但终归是有区别滴(存在即合理嘛)，两者之间的不同可以总结为MSE计算方便利于模型的学习，MAE对异常点具有更高的鲁棒性但在损失较小时仍有较大梯度，求解效率低。 下面详细介绍L0L1L2范数以及它们作为正则项的应用。 一般正则项是以下公式的形式： $$\sum_{n=1}^{N}\left.\middle|t_n-\bf w^T \phi(\bf x_n)\middle|\right.^k+\lambda\sum_{j=1}^M\left.\middle|w_j\middle|\right.^q$$ L0范数L0范数是指向量中非0的元素的个数。 使用L0范数的意义在于希望向量Ｗ中的大部分元素都是0，即让W是稀疏的。(关于稀疏的概念可以看稀疏编码－Sparse coding) 需要注意的是：1.L0很难优化求解(NP难问题)2.虽然“稀疏性”的最直接测度标准是 “L0” 范式，但这是不可微的，而且通常很难进行优化，所以普遍做法是采用L0的最优凸近似L1范数来替代L0。 所以我们才有了L1范数～ L1范数L1范数是指向量中各个元素的绝对值之和,又称曼哈顿距离，马氏距离（Manhattan distance）。 L1正则算子可以实现稀疏，因此又被称为“稀疏规则算子”。而在应用中，也多采用L1正则项实现模型的稀疏性。那么问题来了，L0正则算子具有明显的稀疏约束，而为什么要使用L1正则算子却不用L0呢？ 主要原因有两个，一是L1可以确实实现稀疏约束，二是L0不可导难以优化求解。 关于L1为什么能够实现稀疏，简单解释是因为任何的规则化算子，如果它在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。，直观一点讲的话，L1范数更加倾向于选择一些数目较少的较大的值(换句话讲选择较多的零)。而从优化/先验知识/数值计算的角度更为详细地去理解L1的稀疏性，请移步『科学计算』L0、L1与L2范数理解。 这里插一句，如果数据集中某些特征值很大，而经验风险使用L1的话，L1倾向于选择更大的特征，这些特征会掩盖其他特征间的邻近关系。 L2范数L2范数是指向量中各元素的平方和然后开根号。在回归中，被称作“岭回归”(Ridge Regression)，也叫“权值衰减”(Weight Decay)，可以理解为欧几里得距离(Euclid distance)。 在正则项中，L2范数的作用主要是用于防止数据过拟合，提升模型的泛化能力，同时改善ill-posed问题。 L2为什么会防止过拟合？L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，W中会出现许多接近0但不是0的元素(如果是0就变成L1了)，这些特征都会接近于0，这些小的权重参量会削弱数据中某些特征值较大的数据的作用，使得模型的泛化性能更好。。 对于改善ill-posed的问题，从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。 为什么L2会改善ill-posed的问题呢？简单地说一下，系统的解析解可以表示为：$$\hat w = \left(X^TX\right)^{-1}X^T\rm y $$当系统是ill-conditioned时，样本数量小于样本的维度，从解方程的角度说方程个数小于未知量个数，矩阵$X^TX$是奇异矩阵，即$X^TX$不可逆，所以$w$是不可求的。L2正则在此处就大显神威了，加上L2正则项之后，解变成了：$$\hat w = \left(X^TX-\lambda I\right)^{-1} X^T\rm y$$ 上面这个式子是可以求逆的，并且$\lambda$的引入会改善condition number，从而改善了ill-posed的问题，与此同时，求解的收敛速度也增加了。 参考资料[1] 深度学习——L0、L1及L2范数[2] 曼哈顿距离，欧式距离，余弦距离 [3] 机器学习中的范数规则化之（一）L0、L1与L2范数[4] 机器学习——几种距离度量方法比较[5] 正则项的理解之正则从哪里来[6] 【直观详解】什么是正则化[7] 机器学习大牛最常用的5个回归损失函数，你知道几个？]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>XJBX</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install]]></title>
    <url>%2F2018%2F08%2F01%2Finstall.html</url>
    <content type="text"><![CDATA[install MATLABUninstall MATLABmain reference:https://ww2.mathworks.cn/help/install/ug/install-and-activate-without-an-internet-connection.html 1matlabroot/bin/deactivate_matlab.sh matlabrootrepresents your own MATLAB ROOT FILE FOLDER. 12rm -rf matlabrootrm -r /home/username/.matlab 12sudo gedit ./.bashrc//delete the imformation about matlab Install MATLABmain reference:https://blog.csdn.net/qq_16234613/article/details/78996565https://blog.csdn.net/u011961856/article/details/79644342https://blog.csdn.net/yusiguyuan/article/details/24269129 1234567mkdir /media/matlab//mount -t 类型 -o 挂接方式 源路径 目标路径sudo mount -t auto -o loop /home/username/Downloads/R2018a_glnxa64_dvd1.iso /media/matlabcd..sudo /media/matlab/installumount /media/matlab 12sudo gedit ./.bashrcexport PATH=$PATH:/usr/local/MATLAB/R2018/bin 参考资料：[1][2][3][4] install latex+sublimeinstall texlive1sudo apt-get install texlive install sublime-text1234567wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -sudo apt-get install apt-transport-httpsecho &quot;deb https://download.sublimetext.com/ apt/stable/&quot; | sudo tee /etc/apt/sources.list.d/sublime-text.listsudo apt-get updatesudo apt-get install sublime-text%% apt-get install latexmk install Package-control12345678import urllib.request,os,hashlib; h = &apos;6f4c264a24d933ce70df5dedcf1dcaee&apos; + &apos;ebe013ee18cced0ef93d5f746d80ef60&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) );by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) install dbus1234567cd ~/下载tar xzvf dbus-python-1.2.4.tar.gzsudo apt-get install libdbus-glib-1-devcd dbus-python-1.2.4./configuremakesudo make install Ubuntu安装Sublime Text 3–解决无法使用搜狗中文输入法安装完成后发现无法在Sublime输入中文，而linux常用输入法是搜狗For Linux输入法解决方法: 创建文件保存下面的代码到文件 sublime_imfix.c ,命令： 1cd ~ &amp;&amp; gedit sublime_imfix.c 代码： 1234567891011121314151617#include &lt;gtk/gtkimcontext.h&gt;void gtk_im_context_set_client_window (GtkIMContext *context, GdkWindow *window)&#123; GtkIMContextClass *klass; g_return_if_fail (GTK_IS_IM_CONTEXT (context)); klass = GTK_IM_CONTEXT_GET_CLASS (context); if (klass-&gt;set_client_window) klass-&gt;set_client_window (context, window); g_object_set_data(G_OBJECT(context),&quot;window&quot;,window); if(!GDK_IS_WINDOW (window)) return; int width = gdk_window_get_width(window); int height = gdk_window_get_height(window); if(width != 0 &amp;&amp; height !=0) gtk_im_context_focus_in(context);&#125; 拷贝保存即可～ 编译为共享库将上一步的代码编译成共享库 libsulime-imfix.so ，命令： 12cd ~gcc -shared -o libsublime-imfix.so sublime_imfix.c `pkg-config --libs --cflags gtk+-2.0` -fPIC 有些童鞋无法编译，出现错误： 123No package &apos;gtk+-2.0&apos; foundsublime_imfix.c:1:30: fatal error: gtk/gtkimcontext.h: 没有那个文件或目录 #include &lt;gtk/gtkimcontext.h&gt; 需要先安装必要的依赖： 1sudo apt-get install libgtk2.0-dev 安装完成后重新编译，可以了吧～ 修改sublime文件然后将 libsublime-imfix.so 拷贝到 sublime_text 所在文件夹，命令： 12cd ~sudo mv libsublime-imfix.so /opt/sublime_text/ 修改Sublime的命令 /usr/bin/subl 的内容: 1sudo gedit /usr/bin/subl 将文件内容： 12#!/bin/shexec /opt/sublime_text/sublime_text &quot;$@&quot; 替换修改为： 12#!/bin/shLD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text &quot;$@&quot; 完成后，在命令行中执行 subl 重启Sublime ，就可以使用搜狗For Linux的中文输入法了～ 保证多种启动方式均可使用搜狗继续图形界面快捷方式 sublime_text.desktop 的修改： sudo gedit /usr/share/applications/sublime_text.desktop 将[Desktop Entry]字段下的字符串： 1Exec=/opt/sublime_text/sublime_text %F 替换修改为： 1Exec=bash -c &quot;LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text %F&quot; 将[Desktop Action Window]字段下的字符串： 1Exec=/opt/sublime_text/sublime_text -n 替换修改为： 1Exec=bash -c &quot;LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text -n&quot; 将[Desktop Action Document]字段下的字符串： 1Exec=/opt/sublime_text/sublime_text --command new_file 替换修改为： 1Exec=bash -c &quot;LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text --command new_file&quot; 参考：http://www.qingzz.cn/ubuntu_sublime_sogou Install Code::Blocks123sudo add-apt-repository ppa:damien-moore/codeblocks-stablesudo apt updatesudo apt install codeblocks codeblocks-contrib 如何在Ubuntu 16.04/17.04上安装Code::Blockshttps://blog.csdn.net/skullsky/article/details/53134114 参考资料：[1] Install CUDA]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sublime]]></title>
    <url>%2F2018%2F07%2F24%2Fsublime.html</url>
    <content type="text"><![CDATA[快速查找与替换多数情况下，我们需要查找文中某个关键字出现的其它位置，这时并不需要重新将该关键字重新输入一遍然后搜索，我们只需要使用Shift + ←/→或Ctrl + D选中关键字，然后F3跳到其下一个出现位置，Shift + F3跳到其上一个出现位置，此外还可以用Alt + F3选中其出现的所有位置（之后可以进行多重编辑，也就是快速替换）。 大小写转换转换为大写：Ctrl+KU转换为小写：Ctrl+KL 参考资料：[1] https://www.learnopencv.com/homography-examples-using-opencv-python-c/[2] https://blog.csdn.net/xuyangcao123/article/details/70916767[3] https://www.zybuluo.com/codeep/note/163962[4] http://blog.sina.com.cn/s/blog_6965d96d0101u98s.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>摸鱼日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单应性变换]]></title>
    <url>%2F2018%2F07%2F23%2Fhomography.html</url>
    <content type="text"><![CDATA[单应性变换(homography)所谓单应性变换就是一个平面到另一个平面的映射关系。 如图，两张图片中的相同的点叫做corresponding points,比如图中红色的两点就是一对corresponding points。单应性(homography)矩阵就是表示从一张图到另一张图的映射关系的变换矩阵。 $$H=\begin{bmatrix}h_{00} &amp; h_{01} &amp; h_{02} \h_{10} &amp; h_{11} &amp; h_{12} \h_{20} &amp; h_{21} &amp; h_{22}\end{bmatrix}$$ $$ \begin{bmatrix}x_1 \\ y_1 \\ 1\end{bmatrix}=H\begin{bmatrix}x_2\\ y_2\\ 1\end{bmatrix}=\begin{bmatrix}h_{00} &amp; h_{01} &amp; h_{02}\h_{10} &amp; h_{11} &amp; h_{12}\h_{20} &amp; h_{21} &amp; h_{22}\end{bmatrix}\begin{bmatrix}x_2\y_2\1\end{bmatrix}$$ 代码实现123456789101112131415161718192021222324252627282930#!/usr/bin/env pythonimport cv2import numpy as np if __name__ == '__main__' : # Read source image. im_src = cv2.imread('book2.jpg') # Four corners of the book in source image pts_src = np.array([[141, 131], [480, 159], [493, 630],[64, 601]]) # Read destination image. im_dst = cv2.imread('book1.jpg') # Four corners of the book in destination image. pts_dst = np.array([[318, 256],[534, 372],[316, 670],[73, 473]]) # Calculate Homography h, status = cv2.findHomography(pts_src, pts_dst) # Warp source image to destination based on homography im_out = cv2.warpPerspective(im_src, h, (im_dst.shape[1],im_dst.shape[0])) # Display images cv2.imshow("Source Image", im_src) cv2.imshow("Destination Image", im_dst) cv2.imshow("Warped Source Image", im_out) cv2.waitKey(0) 单应性变换应用最常见的当属于对文档进行自由变换。如使用手机拍摄文件或者明信片时，由于相机或拍摄角度等因素影响，拍摄的文档并不是矩形，而往往带有一定程度的畸变，此时可以使用单应性变换对文档进行自由变换矫正。此处从网上扒了一个使用PS对名片进行自由变换的例子，其本质就是单应性变换的应用。 拍摄照片： PS自由变换后： 另一个例子： 中间时代广场左上角的广告牌被替换为”Les Horribles Cernettes”的海报。 参考资料[1] https://www.learnopencv.com/homography-examples-using-opencv-python-c/[2] https://blog.csdn.net/xuyangcao123/article/details/70916767[3] markdown公式指导手册[4] http://blog.sina.com.cn/s/blog_6965d96d0101u98s.html]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>XJBX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：Enhancing the Spatial Resolution of Stereo Images using Parallax Prior]]></title>
    <url>%2F2018%2F07%2F22%2FEnhancing_the_Spatial_Resolution_of_Stereo_Images_using_Parallax_Prior.html</url>
    <content type="text"><![CDATA[论文要解决的问题是图像空间超分辨，主要的想法是通过双目视觉的视差先验实现图像的亚像素精度空间超分辨。 文章整体的思路比较清晰，首先将RGB转换到$Yc_bc_r$空间，以分别处理亮度(luminance)和色度(chrominance),。网络由两个子网络组成，第一个网络处理Y通道，得到高分辨率的Y通道图像，第二个处理$C_b$和$C_r$通道，得到高分辨率的彩色图像。 问题： 1.视差先验(parallax prior)是怎么体现的？2.在文章结果部分有和VDSR对比的实验结果，但VDSR是SISR，此处使用stereo的两张图像，这么比公平吗？3.Psycho的stereo方法甚至不如VDSR，这。。。。4.在处理第一个网络的输入时，是把right image进行pixel shift，平移n个pixel，但是每个点对应的偏移量是不一样的，这里怎么处理、]]></content>
      <tags>
        <tag>论文笔记</tag>
        <tag>深度信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：乱七八糟集合]]></title>
    <url>%2F2018%2F06%2F23%2Fpaperreading_luanqibazao.html</url>
    <content type="text"><![CDATA[Enhancing the Spatial Resolution of Stereo Images using Parallax Prior这篇论文的main idea 是使用DPN网络和STN网络结合，针对RGB+NIR融合过程出现匹配不准确的现象提出一种多模态融合方案。 STN首先看什么是STN网络。这里就不看原论文了，在CSDN上找了一些文章。 STN的insight：文章提出的STN的作用类似于传统的矫正的作用。比如人脸识别中，需要先对检测的图片进行关键点检测，然后使用关键点来进行对齐操作。但是这样的一个过程是需要额外进行处理的。[进行什么样的额外处理？]但是有了STN后，检测完的人脸，直接就可以做对齐操作。关键的一点就是这个矫正过程是可以进行梯度传导的。想象一下，人脸检测完了，直接使用ROI pooling取出人脸的feature map，输入STN就可以进行矫正，输出矫正后的人脸。后面还可以再接点卷积操作，直接就可以进行分类，人脸识别的训练。整个流程从理论上来说，都有梯度传导，理论上可以将检测+对齐+识别使用一个网络实现。当然实际操作中可能会有各种trick。 STN网络由Localisation Network ，Grid generator，Sampler，3个部分组成。Localisation Network：该网络就是一个简单的回归网络。将输入的图片进行几个卷积操作，然后全连接回归出6个角度值（假设是仿射变换），2*3的矩阵。【为什么是6个角度？】 Grid generator：网格生成器负责将V中的坐标位置，通过矩阵运算，计算出目标图V中的每个位置对应原图U中的坐标位置。即生成T(G)。 这里的Grid采样过程，对于二维仿射变换（旋转，平移，缩放）来说，就是简单的矩阵运算。上式中，s代表原始图的坐标，t代表目标图的坐标。A为Localisation Network网络回归出的6个角度值。 整个Grid生成过程就是，首先你需要想象上图中V-FeatureMap中全是白色或者全是黑色，是没有像素信息的。也就是说V-FeatureMap还不存在，有的只是V-FeatureMap的坐标位置信息。然后将目标图V-FeatureMap中的比如（0，0）（0，1）……位置的坐标，与2*3变换矩阵运算。就会生成出在原始图中对应的坐标信息，比如（5，0）（5，1）……。这样所有的目标图的坐标都经过这样的运算就会将每个坐标都产生一个与之对应的原图的坐标，即T(G)。然后通过T(G)和原始图U-FeatureMap的像素，将原始图中的像素复制到V-FeatureMap中，从而生成目标图的像素。【这步操作有点骚，为什么不生成从U到V的放射变换矩阵？】 Sampler的存在使得STN可微。 STN网络是一个CONV还是FC这个网络会有几个隐层，这些隐层可以是全连接层，也可以是卷积层。也就是说，STN是一种流程思想。 这里没看明白。但后一层，需要做回归，因为要输出变换参数的值。 参考资料[1]https://blog.csdn.net/qq_39422642/article/details/78870629，此博客中的连接很有用。https://blog.csdn.net/qq_14845119/article/details/79510714 DPNBaseline LR中的backward warp很有意思。有点STN中反step3的影子。 发现自己的一个概念一直用错了。右图的视差dr，应用到左图，可以重建出右图。 原文中说，输出四个不同尺度（disp4~disp1）的视差图，它们前者是后者的两倍。尽管只有单张输入图片，该网络能够输出不同尺度上的两个视差图——left-to-right 和 right-to-left。 上面这张图是博客作者自己画的？反正没找到论文作者的版本。网络输出两个视差图是毋庸置疑的，文章material awareness中示意图也说明了这点。为什么能够输出两个视差图呢？ 参考资料：这一篇是DPN的一个baseline LR http://yyliu.cn/post/750ec7f0.html下面两篇是翻译：http://www.yyliu.cn/post/c68cf6db.htmlhttps://zhuanlan.zhihu.com/p/29528596这篇是概括性解读+codehttps://blog.csdn.net/lvhao92/article/details/76586101 Deep Material-aware Cross-spectral Stereo MatchingIntroduction 混合相机系统的align，如果使用Beam filter,会有长时间曝光造成blur.而使用stereo合一在align的同时获得depth 【相当于初始化depth用的是dpn单目方式】 作者让DPN只学深度，使用了symmetric的结构方式STN学习几何信息。 作者采集了数据集，并且自己label 了material segment进行train STN。 【看到这里作者的STN网络是不能借鉴了，猜测借鉴的部分还有wrap等】 Related Work 已有的交叉模态立体匹配方案只考虑了feature和region没有考虑material awareness 已有的非监督深度估计方案只考虑了RGB而没有考虑光谱，还有非漫发射。 【看到这里想到我们的方案是想让两路的图像尽可能接近 采用类DPN方案：可以得到disparity–&gt;align–&gt;enhance(这里enhance与求disp有冲突吗？没有。)求disp的时候可以算一个loss，这个loss主体是||L’-L||_2+||R’-R||_2；enhance的时候loss主体是？？？Enhance需要找1、jiangzhu的论文2、今年CVPR陈畅师兄介绍的zero-shot增强的论文。3、思考数据集怎么做？我们是没有label的。】 Simultaneous Disparity Estimation and Spectral Translation 文字贼有意思，simultaneously to respectively learn disparity and spectral translation 看到他用STN把RGB合成pseudo NIR，我在考虑是否应该使用CNN来把spectral image转化为灰度图像。那么这里为什么不直接用光谱响应曲线呢？考虑如下：1、合成后的灰度图像并不很好。【其实两幅图像的拍摄角度不同，光线也不会一样，所以导致spectral image合成后的图像也并不会完全一样。但是这个差别是否可以忽略不计呢?】 reprojection error 重投影误差 出现了！出现了！使用STN进行transform(wrap)。。。吗【又看了一遍DPN后发现并不是，STN的作用：“在训练过程中，网络学习通过对相反的图片采样来生成图片。我们的成像模型，采用来自空间变形网络（spatial transformer network，STN），通过视差图对输入图像取样。STN采用双线性采样，它的输出像素是四个输入像素的权重相加 。与其他方法相比，双线性采样局部完全可微，无缝集成到我们的全卷积网络架构。意味着不需要单纯化或近似我们的损失函数。”】 Spectral Translation Network 包括局部滤波，白平衡，曝光校正。 pseudo NIR 由RB白平衡G(\theta)和像素值F(\theta)加权得到。 F(\theta)的权值由Filter Generating Network FGN学习得到。 FGN的结构和DPN一样。此处为了不让STN学习到disp信息，所以使用了一个left-right symmetric filtering kernels(symmetric CNN)，即FGN。FGN可以平等地对待左右图的每一个像素并且不对输入产生shift【这里symmetric cnn是什么，为什么可以不学到disp信息？为啥对称了就学不到了？】【STN除了FGN以外还有什么组成】 Incorporating Material-aware Confidence into Disparity Prediction Network 提出一个问题：material awareness和disp prediction没有结合 解决方法：把material-aware confidence加到DPN loss中去。 方案1：把确定区域的disp传播到不确定性区域，（利用确定的disp求不确定的disp），为此，作者提出了一个新的confidence-weighted smoothing technique 方案2：改写DPN LOSS，增加material-specific alignment and smoothness losses 方案1+方案2.Done！ smooth是一种预测未知区域disp的方法（这么讲好有道理） 方案1：简单来说就是对unreliable区域的disp进行类插值预测。 方案2：改造DPN loss貌似很复杂，不看了。 随后作者举例说明怎么使用方案1 2 作者介绍了数据集 Experiment Result 使用了Deeplab net进行fine-tune，用于 material awareness 本文的对比对象是 CMA/ ANCC/ DASC /SIFT FLOW/ ablation study 看到最后有个问题，本文中DPN的输入为什么会画成两个输入的形式？并且输出为两个disp。这里的问题还是original 的DPN没有搞清楚。 20180917再次阅读。 主要阅读method部分。追求技术细节。 首先，DPN的作用是求视差，这里没有问题。关于其约束，使用了重投影误差，查了一下重投影误差并没太仔细看，觉得这里可以先简单理解为左右一致性。其loss设置的比较复杂，但感觉基本就第一项在起作用。 考虑其训练过程和测试过程。]]></content>
      <tags>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟利用Github+Hexo搭建个人博客!]]></title>
    <url>%2F2018%2F06%2F22%2Fgithub%2Bhexo.html</url>
    <content type="text"><![CDATA[前言本文记录了使用github+hexo搭建个人博客的过程以及搭建过程中遇到的一些问题。 关于使用hexo搭建博客，hexo官网已经有了很详细的介绍，但官方文档的介绍往往是力求全面详尽而显得过于冗长(尽管hexo已经够简洁了)，下面介绍如何以最快的速度实现一个“最小hexo系统”。 搭建环境为Ubuntu16.04.(Windows/Mac/Linux等其他环境请参考hexo官网) 安装hexo 安装Git： 1$ sudo apt-get install git-core hexo是基于Node.js的静态博客，需要使用node.js里的npm工具，所以接下来下载Node.js。以下两种方式二选一即可。 12$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh$ wget -qO- https://raw.github.com/creationix/nvm/master/install.sh | sh 下载完成后，重启终端并执行下列命令 1$ nvm install stable 至此，我们完成了安装hexo的准备工作。而安装hexo呢，只需一个命令：4. 安装hexo 1$ npm install hexo-cli -g hexo安装完毕！ 发布你的第一篇文章 初始化hexo 12345$ hexo i blog //init的缩写 blog是项目名$ cd blog //切换到站点根目录$ npm install$ hexo g //generetor的缩写$ hexo s //server的缩写 创建第一篇文章 12hexo new mypaper.md//文章会生成在/blog/sorce/_post/下 本地服务器访问网页打开浏览器输入localhost:4000查看网页效果，当然，此时只是默认主题，如果不喜欢可以使用next，这是一个界面简洁且功能丰富的主题。 设置github仓库在github上新建一个repository并且务必命名为username.github.io。修改本地blog根目录下的配置文件_config.xml: 1234deploy: type: git repo: git@github.com:username/username.github.io.git branch: master 安装插件 1$ npm install hexo-deployer-git --save 将博客部署到github上 123hexo cleanhexo g //generatehexo d //deploy 花里胡哨的东西首页只显示「摘要」在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法： 1.在文章中使用 &lt;!-- more --&gt; 手动进行截断，Hexo 提供的方式 「推荐」2.在文章的 front-matter 中添加 description，并提供文章摘录 3.自动形成摘要，在 主题配置文件 中添加： 123auto_excerpt: enable: true length: 150 默认截取的长度为 150 字符，可以根据需要自行设定 建议使用 &lt;!-- more --&gt;（即第一种方式），除了可以精确控制需要显示的摘录内容以外， 这种方式也可以让 Hexo 中的插件更好的识别。 尝试了第一二种方式，但是第二种方式在超过一行的情况下不work，还没有找到解决方法。 ————20190618更新———— 页面点击小红心将 love.js 文件添加到 \themes\next\source\js\src 文件目录下。love.js: 1!function(e,t,a)&#123;function r()&#123;for(var e=0;e&lt;n.length;e++)n[e].alpha&lt;=0?(t.body.removeChild(n[e].el),n.splice(e,1)):(n[e].y--,n[e].scale+=.004,n[e].alpha-=.013,n[e].el.style.cssText=&quot;left:&quot;+n[e].x+&quot;px;top:&quot;+n[e].y+&quot;px;opacity:&quot;+n[e].alpha+&quot;;transform:scale(&quot;+n[e].scale+&quot;,&quot;+n[e].scale+&quot;) rotate(45deg);background:&quot;+n[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;var n=[];e.requestAnimationFrame=e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;,function(e)&#123;var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),function()&#123;var a=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;a&amp;&amp;a(),function(e)&#123;var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,n.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;),t.body.appendChild(a)&#125;(e)&#125;&#125;(),r()&#125;(window,document); 找到 \themes\next\layout_layout.swing 文件， 在文件的后面， 标签之前 添加以下代码： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 背景的设置将 particle.js 文件添加到 \themes\next\source\js\src 文件目录下。 1!function()&#123;function n(n,e,t)&#123;return n.getAttribute(e)||t&#125;function e(n)&#123;return document.getElementsByTagName(n)&#125;function t()&#123;i=a.width=window.innerWidth||document.documentElement.clientWidth||document.body.clientWidth,c=a.height=window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight&#125;function o()&#123;d.clearRect(0,0,i,c);var n,e,t,a,m,r,y=[x].concat(w);w.forEach(function(o)&#123;for(o.x+=o.xa,o.y+=o.ya,o.xa*=o.x&gt;i||o.x&lt;0?-1:1,o.ya*=o.y&gt;c||o.y&lt;0?-1:1,d.fillRect(o.x-.5,o.y-.5,1,1),e=0;e&lt;y.length;e++)n=y[e],o!==n&amp;&amp;null!==n.x&amp;&amp;null!==n.y&amp;&amp;(a=o.x-n.x,m=o.y-n.y,r=a*a+m*m,r&lt;n.max&amp;&amp;(n===x&amp;&amp;r&gt;=n.max/2&amp;&amp;(o.x-=.03*a,o.y-=.03*m),t=(n.max-r)/n.max,d.beginPath(),d.lineWidth=t/2,d.strokeStyle=&quot;rgba(&quot;+u.c+&quot;,&quot;+(t+.2)+&quot;)&quot;,d.moveTo(o.x,o.y),d.lineTo(n.x,n.y),d.stroke()));y.splice(y.indexOf(o),1)&#125;),l(o)&#125;var i,c,a=document.createElement(&quot;canvas&quot;),u=function()&#123;var t=e(&quot;script&quot;),o=t.length,i=t[o-1];return&#123;l:o,z:n(i,&quot;zIndex&quot;,-1),o:n(i,&quot;opacity&quot;,.5),c:n(i,&quot;color&quot;,&quot;0,0,0&quot;),n:n(i,&quot;count&quot;,99)&#125;&#125;(),m=&quot;c_n&quot;+u.l,d=a.getContext(&quot;2d&quot;),l=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(n)&#123;window.setTimeout(n,1e3/45)&#125;,r=Math.random,x=&#123;x:null,y:null,max:2e4&#125;;a.id=m,a.style.cssText=&quot;position:fixed;top:0;left:0;z-index:&quot;+u.z+&quot;;opacity:&quot;+u.o,e(&quot;body&quot;)[0].appendChild(a),t(),window.onresize=t,window.onmousemove=function(n)&#123;n=n||window.event,x.x=n.clientX,x.y=n.clientY&#125;,window.onmouseout=function()&#123;x.x=null,x.y=null&#125;;for(var w=[],y=0;u.n&gt;y;y++)&#123;var s=r()*i,f=r()*c,h=2*r()-1,g=2*r()-1;w.push(&#123;x:s,y:f,xa:h,ya:g,max:6e3&#125;)&#125;setTimeout(function()&#123;o()&#125;,100)&#125;(); 找到 \themes\next\layout_layout.swing 文件， 在文件的后面，L2Dwidget.init({"model":"hijiki","bottom":-30,"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});标签之前 添加以下代码： 12&lt;!-- 背景动画 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/particle.js&quot;&gt;&lt;/script&gt; 给 Github 添加 README默认情况下，Github中每一个项目，我们希望有一份 README.md 的文件来作为项目的说明，但是我们在项目根目录下的 blog\source 目录下创建一份 README.md 文件，写好说明介绍，部署的时候，这个 README.md 会被 hexo 解析掉，而不会被解析到 Github 中去的。正确的解决方法其实很简单：方法1：把 README.md 文件的后缀名改成 “MDOWN” 然后扔到blog/source文件夹下即可，这样 hexo 不会解析，Github 也会将其作为 MD 文件解析。方法2：解决方法很简单，在站点配置文件中，搜索 skip_render:，在其冒号后加一个空格然后加上 README.md 即可。 ##### 实现guestbook留言板功能 进入到博客的根目录，运行命令： 1hexo new page guestbook ##### 添加Local search功能 安装搜索插件： 1hexo-generator-searchdb 在博客根目录下执行以下命令： 1$ npm install hexo-generator-searchdb --save 配置博客安装完成，编辑博客配置文件：_config.yml 12345search: path: search.xml field: post format: html limit: 10000 配置主题Next 主题自带搜索设置，编辑主题配置文件：_config.yml 找到文件中 Local search 的相关配置，设为 true 123# Local searchlocal_search: enable: true hexo 重新部署 hexo本地图片插入关于图片插入，耗费了我一些时间。最初我是在/source/文件夹下建立了一个upload_image/用来存放图片，在markdown中调用时是![image](../../upload_image/image.png)这种形式，可以看出，需要向上跳两级目录，才能找到对应的图像。后来又尝试把图像直接放在与.md文件同一目录下，要不然是在编辑markdown时不显示，要不然是生成的静态网页不显示。找了好久终于在Hexo 图片插入找到了解决方法，需要安装 1npm install https://github.com/CodeFalling/hexo-asset-image --save 这样一个插件。方可以使用相对路径的图片，既可以在markdown中显示，也可以在网页中显示。 参考资料[1] 大道至简——Hexo简洁主题推荐[2]Hexo-NexT搭建个人博客（二）[3]Hexo 搭建博客的个性化设置二[4]Hexo NexT 主题添加留言本页面[5]hexo - Next 主题添加搜索功能]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>摸鱼日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用神经网络进行图像超分辨踩的坑]]></title>
    <url>%2F2018%2F05%2F02%2FSubpixelTrick.html</url>
    <content type="text"><![CDATA[1、matlab的bicubic结果会有小于0或者大于255的情况出现。 Image interpolation wrong for pixel values exceeding vmax #8631MATLABresize函数中也说明了： 1234% For bicubic interpolation, the output image may have some values% slightly outside the range of pixel values in the input image. This% may also occur for user-specified interpolation kernels.% 2、神经网络训练出来的结果也会有小于0或者大于255的情况出现 Also remember that loss function is based on scaled images (pixel values between 0 and 1), so the predicted residual has to be scaled back by 255 before it is added to low resolution image. If some pixels become negative or go above 255, we return them to 0 and 255 respectively before saving. 解决方法：*make sure no pixels are outside [0, 255] interval* 参考1.https://www.cntk.ai/pythondocs/CNTK_302A_Evaluation_of_Pretrained_Super-resolution_Models.html2.https://cntk.ai/pythondocs/CNTK_302B_Image_Super-resolution_Using_CNNs_and_GANs.html3.https://blog.csdn.net/Autism_/article/details/794017984.https://github.com/matplotlib/matplotlib/issues/8631]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>XJBX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Life is short you need python]]></title>
    <url>%2F2018%2F04%2F22%2FpythonTrick.html</url>
    <content type="text"><![CDATA[关于if not x:&amp;if x is not None&amp;if not x is None代码中经常会有变量是否为None的判断，有三种主要的写法： 第一种是if x is None； 第二种是 if not x：； 第三种是if not x is None（这句这样理解更清晰if not (x is None)） 。 如果你觉得这样写没啥区别，那么你可就要小心了，这里面有一个坑。先来看一下代码： 123456789101112&gt;&gt;&gt; x = 1 &gt;&gt;&gt; not x False &gt;&gt;&gt; x = [1] &gt;&gt;&gt; not x False &gt;&gt;&gt; x = 0 &gt;&gt;&gt; not x True &gt;&gt;&gt; x = [0] # You don&apos;t want to fall in this one. &gt;&gt;&gt; not x False 在python中 None, False, 空字符串””, 0, 空列表[], 空字典{}, 空元组()都相当于False ，即： 1not None == not False == not &apos;&apos; == not 0 == not [] == not &#123;&#125; == not () 因此在使用列表的时候，如果你想区分x==[]和x==None两种情况的话, 此时if not x:将会出现问题： 1234567891011121314151617181920&gt;&gt;&gt; x = [] &gt;&gt;&gt; y = None &gt;&gt;&gt; &gt;&gt;&gt; x is None False &gt;&gt;&gt; y is None True &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; not x True &gt;&gt;&gt; not y True &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; not x is None &gt;&gt;&gt; True &gt;&gt;&gt; not y is None False &gt;&gt;&gt; 也许你是想判断x是否为None，但是却把x==[]的情况也判断进来了，此种情况下将无法区分。对于习惯于使用if not x这种写法的pythoner，必须清楚x等于None, False, 空字符串””, 0, 空列表[], 空字典{}, 空元组()时对你的判断没有影响才行。 而对于if x is not None和if not x is None写法，很明显前者更清晰，而后者有可能使读者误解为if (not x) is None，因此推荐前者，同时这也是谷歌推荐的风格 结论： if x is not None是最好的写法，清晰，不会出现错误，以后坚持使用这种写法。 使用if not x这种写法的前提是：必须清楚x等于None, False, 空字符串””, 0, 空列表[], 空字典{}, 空元组()时对你的判断没有影响才行。 Python if 和 for 的多种写法1.not in 123456&gt;&gt;&gt; a=2&gt;&gt;&gt; a not in [2,3,4]False&gt;&gt;&gt; a in [2,3,4] True 2.c if a else b #这里注意，一定要有b,而且b不能为pass 1234567&gt;&gt;&gt; a=3 if 2&gt;3 else 4&gt;&gt;&gt; a4&gt;&gt;&gt; a=3 if 2&lt;3 else 4 &gt;&gt;&gt; a3 3.[fun(a) for a in […]] 12&gt;&gt;&gt; [a+1 for a in [2,3,4,5,6]][3, 4, 5, 6, 7] 4.a,b=b,a 1234567&gt;&gt;&gt; a=1&gt;&gt;&gt; b=2&gt;&gt;&gt; a,b=b,a&gt;&gt;&gt; a2&gt;&gt;&gt; b1 5.’内容’.join([string array]) 12345678910&gt;&gt;&gt; &apos;.&apos;.join[2,3,4,5,6] Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &apos;builtin_function_or_method&apos; object has no attribute &apos;__getitem__&apos;&gt;&gt;&gt; &apos;.&apos;.join([2,3,4,5,6]) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: sequence item 0: expected string, int found&gt;&gt;&gt; &apos;.&apos;.join([&apos;2&apos;,&apos;3&apos;,&apos;4&apos;,&apos;5&apos;,&apos;6&apos;]) &apos;2.3.4.5.6&apos; 数组顺序翻转排序：https://blog.csdn.net/u014292358/article/details/79503397OpenCV与Python之图像的读入与显示以及利用Numpy的图像转换：https://www.cnblogs.com/visionfeng/p/6094423.html # Convert from RGB -&gt; BGR input_a = input_a[..., [2, 1, 0]] input_b = input_b[..., [2, 1, 0]]###参考资料：[1] python代码if not x: 和if x is not None:和if not x is None:使用[2] [python里面的几个用法，not in，c if a else b，[fun(a) for a in […]] , a,b=b,a,’内容’.join(string array)](https://blog.csdn.net/u013176681/article/details/53995190)[3] Python if 和 for 的多种写法]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>摸鱼日志</tag>
        <tag>XJBX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[光谱响应与量子效率]]></title>
    <url>%2F2018%2F04%2F18%2FQE.html</url>
    <content type="text"><![CDATA[##光谱响应光谱响应单位为A/W,直观物理意义是单位功率下产生的电流响应。 ##量子效率量子效率分为外量子效率和内量子效率一般量子效率指外量子效率，是指单位时间内外电路中产生的电子数与单位时间内的入射单色光子数之比。内量子效率的定义为：单位时间内外电路中产生的电子书与单位时间内的入射（有效）单色光子数之比。比较两个定义，可以看出内量子效应强调的是有效两字。那么无效的部分就是指因为反射或者投射等不被sensor所接收的光子了。 ##二者关系量子效率定义中，单位时间内的电子数 即为 的电流 。光谱响应定义中，也有关于电流的概念：单位功率下产生的电流。 所以，量子效率 = (1240 * 光谱响应)/响应波长 E=hv=hc/λ, h和c是常量，把数值带进去。注意：因为最后E的单位是eV，λ的单位是nm，所以在中间会涉及单位的转换。h = 6.626196×10^-34 J.s， c = 3×10^8 m/s， 上面的单位是ev，所以还要除以1.6×10^-19 c，λ的单位nm换成m还要乘以1×10^9，最后就得到1240/λ eV 有时候会把两个概念混为一谈 光谱响应 量子效率 光谱响应指光阴极量子效率与入射波长之间的关系光谱响应表示不同波长的光子产生电子-空穴对的能力。定量地说，太阳电池的光谱响应就是当某一波长的光照射在电池表面上时，每一光子平均所能收集到的载流子数。太阳电池的光谱响应又分为绝对光谱响应和相对光谱响应。各种波长的单位辐射光能或对应的光子入射到太阳电池上，将产生不同的短路电流，按波长的分布求得其对应的短路电流变化曲线称为太阳电池的绝对光谱响应。如果每一波长以一定等量的辐射光能或等光子数入射到太阳电池上，所产生的短路电流与其中最大短路电流比较，按波长的分布求得其比值变化曲线，这就是该太阳电池的相对光谱响应。但是，无论是绝对还是相对光谱响应，光谱响应曲线峰值越高，越平坦，对应电池的短路电流密度就越大，效率也越高。 ##参考资料[1] http://muchong.com/html/201309/6391286.html[2] https://baike.baidu.com/item/%E9%87%8F%E5%AD%90%E6%95%88%E7%8E%87/3641896?fr=aladdin[3] https://www.jianshu.com/p/0864630e5fe4]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>XJBX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MATLAB读取曲线图并重新绘制]]></title>
    <url>%2F2018%2F04%2F08%2FMatlabRePlot.html</url>
    <content type="text"><![CDATA[最近需要从datasheet中读取某款相机的光谱响应曲线，发现下面这段代码^1。但是最后发现和自己的需求并不匹配，自己是要最终求出曲线的函数，完成由离散的点到数学公式的转换过程。这个小demo能够实现的仅仅是re-draw the pictures.仍旧记之。 12345678910111213141516171819202122232425I = imread('target.png');%读取处理好的图片，必须是严格坐标轴线为边界的图片I=rgb2gray(I); %灰度变化I(I&gt;200)=255; %二值化I(I&lt;=200)=0; %二值化imshow(I) %显示图片figure;[y,x] = find(I==0); %找出曲线的像素位置y = max(y) -y; %将屏幕坐标转换为有手系迪卡坐标plot(x,y,'r.','markersize',2)%显示转换后的图像[Xx,Yy]= ginput(2); % 读取真是坐标左上角和右下角的两点min_x = min(Xx);max_x = max(Xx);min_y= min(Yy);max_y = max(Yy);% x1 = (x-Xx(1))*(max_x-min_x)/(Xx(2)- Xx(1))+min_x;% y1 = (y-Yy(1))*(min_y-max_y)/(Yy(2)- Yy(1))+max_y;%% 坐标变化，如果坐标原点不为0，则需在该轴加上省去的坐标轴数xo = 0;%原始图像起点坐标xyo = 0;%原始图像起点坐标xxl = 3; %原始图像x轴长yl=25; %原始图像y轴长x1 = (x - min(Xx))*xl/(max(Xx)-min(Xx))+xo ;%数据点x值y1 = (y - min(Yy))*yl/(max(Yy)-min(Yy))+yo;%数据点y值plot(x1,y1,'r.','markersize',2)axis([0,3,0,25]) 原图：效果图：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
        <tag>XJBX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello Hexo!]]></title>
    <url>%2F2018%2F03%2F27%2Fhello-hexo.html</url>
    <content type="text"><![CDATA[hello, welcome!]]></content>
      <tags>
        <tag>摸鱼日志</tag>
      </tags>
  </entry>
</search>
